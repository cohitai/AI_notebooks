{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM3DDRgzXEbis+DVb2cpAuS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d1d3b09cf1064675923821e8dadba729": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cdc5905f2a054a1f8ff868632eea4c78",
              "IPY_MODEL_6206aabf38474507bc92d99872796328",
              "IPY_MODEL_18eb6c034dd1477e83acdbaa73330b40"
            ],
            "layout": "IPY_MODEL_6902e8f3d6b2447586223271a8e1cfd0"
          }
        },
        "cdc5905f2a054a1f8ff868632eea4c78": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_55f26aa3a71549d7bea921f6349c0b9d",
            "placeholder": "​",
            "style": "IPY_MODEL_7198543a611442358e41b29263ae2c85",
            "value": "Epoch 1: 100%"
          }
        },
        "6206aabf38474507bc92d99872796328": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d5b74588877949dcb645e0524c811fec",
            "max": 782,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c2c3ab66678440b089b60143155ead76",
            "value": 782
          }
        },
        "18eb6c034dd1477e83acdbaa73330b40": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5f01630f1e174d2ba8acfd3c97e7287b",
            "placeholder": "​",
            "style": "IPY_MODEL_915d687e2c124ae5837e148b462fd2f7",
            "value": " 782/782 [07:06&lt;00:00,  2.27batch/s, loss=3.41e+3]"
          }
        },
        "6902e8f3d6b2447586223271a8e1cfd0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "55f26aa3a71549d7bea921f6349c0b9d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7198543a611442358e41b29263ae2c85": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d5b74588877949dcb645e0524c811fec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c2c3ab66678440b089b60143155ead76": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5f01630f1e174d2ba8acfd3c97e7287b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "915d687e2c124ae5837e148b462fd2f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "54f73c0eeb114a21885f498ecbad5457": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ca5765510cdd44e19df43f086c7eba1a",
              "IPY_MODEL_fd48d1c48e9e47b9a00c58c5666d6fa0",
              "IPY_MODEL_973eff8b283648508c85b40fec4c118e"
            ],
            "layout": "IPY_MODEL_c8b51952803f4b76b580504c85c7386f"
          }
        },
        "ca5765510cdd44e19df43f086c7eba1a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d546cba5c5304ecb98ac59314bddb267",
            "placeholder": "​",
            "style": "IPY_MODEL_45d4cbf710724c9f8f8b6066519cb345",
            "value": "Epoch 2: 100%"
          }
        },
        "fd48d1c48e9e47b9a00c58c5666d6fa0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_247416a20a464edaa08092204fc645ca",
            "max": 782,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1319a5e7e8534d8ba1267a34fd895a31",
            "value": 782
          }
        },
        "973eff8b283648508c85b40fec4c118e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cfedb7ecee9b49a38626f559f11494a5",
            "placeholder": "​",
            "style": "IPY_MODEL_a77b412e6c7f4a7f9bcf2bedf0265c6a",
            "value": " 782/782 [07:06&lt;00:00,  2.28batch/s, loss=3.81e+3]"
          }
        },
        "c8b51952803f4b76b580504c85c7386f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d546cba5c5304ecb98ac59314bddb267": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "45d4cbf710724c9f8f8b6066519cb345": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "247416a20a464edaa08092204fc645ca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1319a5e7e8534d8ba1267a34fd895a31": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cfedb7ecee9b49a38626f559f11494a5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a77b412e6c7f4a7f9bcf2bedf0265c6a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9b82f1b604fc45ff9a83da64d4f32672": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_faf85789146943e28adc90a996591227",
              "IPY_MODEL_645f94885bdf4ab9998022b04df33c3b",
              "IPY_MODEL_114dfa6aedd045f98d46293c955dfd5c"
            ],
            "layout": "IPY_MODEL_3c10374eaa6a4f79b3a9a01f0bcf4451"
          }
        },
        "faf85789146943e28adc90a996591227": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4a71e0058b7e4827a2c08550f27a45ed",
            "placeholder": "​",
            "style": "IPY_MODEL_0b0ba60c97cc49fa862500dc2e1d5d90",
            "value": "Epoch 3:  75%"
          }
        },
        "645f94885bdf4ab9998022b04df33c3b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5ed195c423f04962831c74a84886500f",
            "max": 782,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cd024b6e2c124b48bc3c25093b27f6bd",
            "value": 590
          }
        },
        "114dfa6aedd045f98d46293c955dfd5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e0a366d23d1646e18d4ca9a8a9c6d139",
            "placeholder": "​",
            "style": "IPY_MODEL_6aa4a8d0e22a475d86d5c6a550e23fbb",
            "value": " 590/782 [05:21&lt;01:44,  1.84batch/s, loss=1.34e+4]"
          }
        },
        "3c10374eaa6a4f79b3a9a01f0bcf4451": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4a71e0058b7e4827a2c08550f27a45ed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0b0ba60c97cc49fa862500dc2e1d5d90": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5ed195c423f04962831c74a84886500f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cd024b6e2c124b48bc3c25093b27f6bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e0a366d23d1646e18d4ca9a8a9c6d139": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6aa4a8d0e22a475d86d5c6a550e23fbb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cohitai/AI_notebooks/blob/main/VAE/VAE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Variational AutoEncoder = (encoder= Q(Z|X)=Recognition model, decoder=P(X|Z)=Generative model)+ Variational loss.\n",
        "\n",
        " Variational loss = ELBO (evidence lower bound). \n",
        " \n",
        " optimization is totally Stochastic, only a single sample at a time .\n",
        "\n"
      ],
      "metadata": {
        "id": "TjIgUpV0H-Nq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ToDO: visualizations of the latent space, KL divergence explanations, Generative data. \n",
        "\n",
        "# Go over these: \n",
        "\n",
        "# https://avandekleut.github.io/vae/\n",
        "\n",
        "# https://github.com/AntixK/PyTorch-VAE (many different types of VAEs)\n",
        "# https://www.jeremyjordan.me/variational-autoencoders/\n",
        "# https://gaussian37.github.io/deep-learning-chollet-8-4/\n",
        "# https://medium.com/@arjun.majumdar/variational-autoencoder-cifar-10-tf2-9ed1155771e1\n",
        "# https://github.com/arjun-majumdar/Autoencoders_Experiments/blob/master/Variational_AutoEncoder_CIFAR10_TF2.ipynb\n",
        "# https://towardsdatascience.com/variational-autoencoder-demystified-with-pytorch-implementation-3a06bee395ed\n",
        "# https://colab.research.google.com/drive/1_yGmk8ahWhDs23U4mpplBFa-39fsEJoT?usp=sharing#scrollTo=xOVursP7lpR-\n",
        "\n",
        "# More ToDo's:\n",
        "# 1. plot the losses.\n",
        "# 2. plot reconstruction versus original images as 2 X M subplot. \n",
        "# 2. Visualization of the latent space from https://avandekleut.github.io/vae/\n",
        "# 3. evaluation data + some kind of accuracy\n",
        "# 4. sum() or mean() in loss , and kl.sum() (probabely) or kl.mean() ?  (stochstic !)\n",
        "# 5. Compare the loss with the pretrained VAE model(Huggingfaces). \n",
        "\n",
        "# Improvements\n",
        "# 0. adjust the learning rate better\n",
        "# 1. Exponential Learning rate ? shadow variables etc \n",
        "# 2. Perceptual Losses for Deep Image Restoration\n",
        "# 3. https://towardsdatascience.com/perceptual-losses-for-image-restoration-dd3c9de4113\n",
        "# 4. Compare the loss with the pretrained VAE model(Huggingfaces). \n",
        "\n",
        "\n",
        "# Theory: main paper on VAE: (Auto-Encoding Variational Bayes) at https://arxiv.org/pdf/1312.6114.pdf\n",
        "\n",
        "# Unet variations\n",
        "# Unet: https://amaarora.github.io/2020/09/13/unet.html\n",
        "# Unet paper: U-Net: Convolutional Networks for Biomedical Image Segmentation (https://arxiv.org/pdf/1505.04597.pdf)\n",
        "\n",
        "# Diffusers on local M1 ANE (transfer to Colab). "
      ],
      "metadata": {
        "id": "VrgX3VIFo8Dk"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch \n",
        "import torchvision\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import os\n",
        "import math\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "print(\"Working directory:\", os.getcwd())\n",
        "device=\"cuda\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rMwmbbQMWjkm",
        "outputId": "e0e00372-57aa-4ef8-f759-45a4bec6ea81"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "Working directory: /content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d-ZChfoGGKpW",
        "outputId": "2ab8c25a-28ab-43b0-9088-95c170eb5722"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri Nov 18 17:34:30 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   55C    P8    10W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Blocks:\n",
        " - DownEncoderBlock2D\n",
        "\n",
        " - UpDecoderBlock2D\n",
        "\n",
        " - ResnetBlock\n",
        "\n",
        " - Downsample2D\n",
        "\n",
        " - Upsample2D\n",
        "\n",
        " - AttentionBlockNew\n",
        "\n",
        " - UNetMidBlock2D\n",
        "\n",
        " - Encoder\n",
        "\n",
        " - Decoder\n"
      ],
      "metadata": {
        "id": "JZ3V7x4oWwCJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DownEncoderBlock2D(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels: int,\n",
        "        out_channels: int,\n",
        "        dropout: float = 0.0, \n",
        "        num_layers: int = 1,\n",
        "        resnet_eps: float = 1e-6,\n",
        "        resnet_time_scale_shift: str = \"default\",\n",
        "        resnet_act_fn: str = \"swish\",\n",
        "        resnet_groups: int = 32,\n",
        "        resnet_pre_norm: bool = True,\n",
        "        output_scale_factor=1.0,\n",
        "        add_downsample=True,\n",
        "        downsample_padding=1,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        resnets = []\n",
        "\n",
        "        for i in range(num_layers):\n",
        "            in_channels = in_channels if i == 0 else out_channels\n",
        "            resnets.append(\n",
        "                ResnetBlock(\n",
        "                    in_channels=in_channels,\n",
        "                    out_channels=out_channels,\n",
        "                    temb_channels=None,\n",
        "                    eps=resnet_eps,\n",
        "                    groups=resnet_groups,\n",
        "                    dropout=dropout,\n",
        "                    time_embedding_norm=resnet_time_scale_shift,\n",
        "                    non_linearity=resnet_act_fn,\n",
        "                    output_scale_factor=output_scale_factor,\n",
        "                    pre_norm=resnet_pre_norm,\n",
        "                )\n",
        "            )\n",
        "\n",
        "        self.resnets = nn.ModuleList(resnets)\n",
        "\n",
        "        if add_downsample:\n",
        "            self.downsamplers = nn.ModuleList(\n",
        "                [\n",
        "                    Downsample2D(\n",
        "                        in_channels, use_conv=True, out_channels=out_channels, padding=downsample_padding, name=\"op\"\n",
        "                    )\n",
        "                ]\n",
        "            )\n",
        "        else:\n",
        "            self.downsamplers = None\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        for resnet in self.resnets:\n",
        "            hidden_states = resnet(hidden_states, temb=None)\n",
        "\n",
        "        if self.downsamplers is not None:\n",
        "            for downsampler in self.downsamplers:\n",
        "                hidden_states = downsampler(hidden_states)\n",
        "\n",
        "        return hidden_states\n",
        "\n",
        "class UpDecoderBlock2D(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels: int,\n",
        "        out_channels: int,\n",
        "        dropout: float = 0.0,\n",
        "        num_layers: int = 1,\n",
        "        resnet_eps: float = 1e-6,\n",
        "        resnet_time_scale_shift: str = \"default\",\n",
        "        resnet_act_fn: str = \"swish\",\n",
        "        resnet_groups: int = 32,\n",
        "        resnet_pre_norm: bool = True,\n",
        "        output_scale_factor=1.0,\n",
        "        add_upsample=True,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        resnets = []\n",
        "\n",
        "        for i in range(num_layers):\n",
        "            input_channels = in_channels if i == 0 else out_channels\n",
        "\n",
        "            resnets.append(\n",
        "                ResnetBlock(\n",
        "                    in_channels=input_channels,\n",
        "                    out_channels=out_channels,\n",
        "                    temb_channels=None,\n",
        "                    eps=resnet_eps,\n",
        "                    groups=resnet_groups,\n",
        "                    dropout=dropout,\n",
        "                    time_embedding_norm=resnet_time_scale_shift,\n",
        "                    non_linearity=resnet_act_fn,\n",
        "                    output_scale_factor=output_scale_factor,\n",
        "                    pre_norm=resnet_pre_norm,\n",
        "                )\n",
        "            )\n",
        "\n",
        "        self.resnets = nn.ModuleList(resnets)\n",
        "\n",
        "        if add_upsample:\n",
        "            self.upsamplers = nn.ModuleList([Upsample2D(out_channels, use_conv=True, out_channels=out_channels)])\n",
        "        else:\n",
        "            self.upsamplers = None\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        for resnet in self.resnets:\n",
        "            hidden_states = resnet(hidden_states, temb=None)\n",
        "\n",
        "        if self.upsamplers is not None:\n",
        "            for upsampler in self.upsamplers:\n",
        "                hidden_states = upsampler(hidden_states)\n",
        "\n",
        "        return hidden_states\n",
        "\n",
        "  \n",
        "    \n",
        "class ResnetBlock(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        *,\n",
        "        in_channels,\n",
        "        out_channels=None,\n",
        "        conv_shortcut=False,\n",
        "        dropout=0.0,\n",
        "        temb_channels=512,\n",
        "        groups=32,\n",
        "        groups_out=None,\n",
        "        pre_norm=True,\n",
        "        eps=1e-6,\n",
        "        non_linearity=\"swish\",\n",
        "        time_embedding_norm=\"default\",\n",
        "        kernel=None,\n",
        "        output_scale_factor=1.0,\n",
        "        use_nin_shortcut=None,\n",
        "        up=False,\n",
        "        down=False,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.pre_norm = pre_norm\n",
        "        self.pre_norm = True\n",
        "        self.in_channels = in_channels\n",
        "        out_channels = in_channels if out_channels is None else out_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.use_conv_shortcut = conv_shortcut\n",
        "        self.time_embedding_norm = time_embedding_norm\n",
        "        self.up = up\n",
        "        self.down = down\n",
        "        self.output_scale_factor = output_scale_factor\n",
        "\n",
        "        if groups_out is None:\n",
        "            groups_out = groups\n",
        "\n",
        "        self.norm1 = torch.nn.GroupNorm(num_groups=groups, num_channels=in_channels, eps=eps, affine=True)\n",
        "\n",
        "        self.conv1 = torch.nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "        if temb_channels is not None:\n",
        "            self.time_emb_proj = torch.nn.Linear(temb_channels, out_channels)\n",
        "        else:\n",
        "            self.time_emb_proj = None\n",
        "\n",
        "        self.norm2 = torch.nn.GroupNorm(num_groups=groups_out, num_channels=out_channels, eps=eps, affine=True)\n",
        "        self.dropout = torch.nn.Dropout(dropout)\n",
        "        self.conv2 = torch.nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "        if non_linearity == \"swish\":\n",
        "            self.nonlinearity = lambda x: F.silu(x)\n",
        "        elif non_linearity == \"mish\":\n",
        "            self.nonlinearity = Mish()\n",
        "        elif non_linearity == \"silu\":\n",
        "            self.nonlinearity = nn.SiLU()\n",
        "\n",
        "        self.upsample = self.downsample = None\n",
        "        if self.up:\n",
        "            if kernel == \"fir\":\n",
        "                fir_kernel = (1, 3, 3, 1)\n",
        "                self.upsample = lambda x: upsample_2d(x, k=fir_kernel)\n",
        "            elif kernel == \"sde_vp\":\n",
        "                self.upsample = partial(F.interpolate, scale_factor=2.0, mode=\"nearest\")\n",
        "            else:\n",
        "                self.upsample = Upsample2D(in_channels, use_conv=False)\n",
        "        \n",
        "        elif self.down:\n",
        "            if kernel == \"fir\":\n",
        "                fir_kernel = (1, 3, 3, 1)\n",
        "                self.downsample = lambda x: downsample_2d(x, k=fir_kernel)\n",
        "            elif kernel == \"sde_vp\":\n",
        "                self.downsample = partial(F.avg_pool2d, kernel_size=2, stride=2)\n",
        "            else:\n",
        "                self.downsample = Downsample2D(in_channels, use_conv=False, padding=1, name=\"op\")\n",
        "\n",
        "        self.use_nin_shortcut = self.in_channels != self.out_channels if use_nin_shortcut is None else use_nin_shortcut\n",
        "\n",
        "        self.conv_shortcut = None\n",
        "    \n",
        "        if self.use_nin_shortcut:\n",
        "            self.conv_shortcut = torch.nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0)\n",
        "\n",
        "    def forward(self, x, temb, hey=False):\n",
        "        h = x\n",
        "\n",
        "        # print(\"LOG resnet_input\",h.size())\n",
        "\n",
        "        # make sure hidden states is in float32\n",
        "        # when running in half-precision\n",
        "        h = self.norm1(h.float()).type(h.dtype)\n",
        "        h = self.nonlinearity(h)\n",
        "\n",
        "        # if self.upsample is not None:\n",
        "        #     x = self.upsample(x)\n",
        "        #     h = self.upsample(h)\n",
        "        # elif self.downsample is not None:\n",
        "        #     x = self.downsample(x)\n",
        "        #     h = self.downsample(h)\n",
        "\n",
        "        h = self.conv1(h)\n",
        "\n",
        "        # print(\"LOG temb:\",temb is None)\n",
        "\n",
        "        # if temb is not None:\n",
        "        #     temb = self.time_emb_proj(self.nonlinearity(temb))[:, :, None, None]\n",
        "        #     h = h + temb\n",
        "\n",
        "        # make sure hidden states is in float32\n",
        "        # when running in half-precision\n",
        "        h = self.norm2(h.float()).type(h.dtype)\n",
        "        h = self.nonlinearity(h)\n",
        "\n",
        "        h = self.dropout(h)\n",
        "        h = self.conv2(h)\n",
        "\n",
        "        if self.conv_shortcut is not None:\n",
        "            x = self.conv_shortcut(x)\n",
        "\n",
        "        out = (x + h) / self.output_scale_factor\n",
        "\n",
        "        return out\n",
        "\n",
        "    def set_weight(self, resnet):\n",
        "        self.norm1.weight.data = resnet.norm1.weight.data\n",
        "        self.norm1.bias.data = resnet.norm1.bias.data\n",
        "\n",
        "        self.conv1.weight.data = resnet.conv1.weight.data\n",
        "        self.conv1.bias.data = resnet.conv1.bias.data\n",
        "\n",
        "        if self.time_emb_proj is not None:\n",
        "            self.time_emb_proj.weight.data = resnet.temb_proj.weight.data\n",
        "            self.time_emb_proj.bias.data = resnet.temb_proj.bias.data\n",
        "\n",
        "        self.norm2.weight.data = resnet.norm2.weight.data\n",
        "        self.norm2.bias.data = resnet.norm2.bias.data\n",
        "\n",
        "        self.conv2.weight.data = resnet.conv2.weight.data\n",
        "        self.conv2.bias.data = resnet.conv2.bias.data\n",
        "\n",
        "        if self.use_nin_shortcut:\n",
        "            self.conv_shortcut.weight.data = resnet.nin_shortcut.weight.data\n",
        "            self.conv_shortcut.bias.data = resnet.nin_shortcut.bias.data\n",
        "            \n",
        "            \n",
        "class Downsample2D(nn.Module):\n",
        "    \"\"\"\n",
        "    A downsampling layer with an optional convolution.\n",
        "    :param channels: channels in the inputs and outputs. :param use_conv: a bool determining if a convolution is\n",
        "    applied. :param dims: determines if the signal is 1D, 2D, or 3D. If 3D, then\n",
        "                 downsampling occurs in the inner-two dimensions.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, channels, use_conv=False, out_channels=None, padding=1, name=\"conv\"):\n",
        "        super().__init__()\n",
        "        self.channels = channels\n",
        "        self.out_channels = out_channels or channels\n",
        "        self.use_conv = use_conv\n",
        "        self.padding = padding\n",
        "        stride = 2\n",
        "        self.name = name\n",
        "\n",
        "        if use_conv:\n",
        "            conv = nn.Conv2d(self.channels, self.out_channels, 3, stride=stride, padding=padding)\n",
        "        else:\n",
        "            assert self.channels == self.out_channels\n",
        "            conv = nn.AvgPool2d(kernel_size=stride, stride=stride)\n",
        "\n",
        "        # TODO(Suraj, Patrick) - clean up after weight dicts are correctly renamed\n",
        "        if name == \"conv\":\n",
        "            self.Conv2d_0 = conv\n",
        "            self.conv = conv\n",
        "        elif name == \"Conv2d_0\":\n",
        "            self.conv = conv\n",
        "        else:\n",
        "            self.conv = conv\n",
        "\n",
        "    def forward(self, x):\n",
        "        assert x.shape[1] == self.channels\n",
        "        if self.use_conv and self.padding == 0:\n",
        "            pad = (0, 1, 0, 1)\n",
        "            x = F.pad(x, pad, mode=\"constant\", value=0)\n",
        "\n",
        "        assert x.shape[1] == self.channels\n",
        "        x = self.conv(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "class Upsample2D(nn.Module):\n",
        "\n",
        "    \"\"\"\n",
        "    An upsampling layer with an optional convolution.\n",
        "    :param channels: channels in the inputs and outputs. :param use_conv: a bool determining if a convolution is\n",
        "    applied. :param dims: determines if the signal is 1D, 2D, or 3D. If 3D, then\n",
        "                 upsampling occurs in the inner-two dimensions.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, channels, use_conv=False, use_conv_transpose=False, out_channels=None, name=\"conv\"):\n",
        "        super().__init__()\n",
        "        self.channels = channels\n",
        "        self.out_channels = out_channels or channels\n",
        "        self.use_conv = use_conv\n",
        "        self.use_conv_transpose = use_conv_transpose\n",
        "        self.name = name\n",
        "\n",
        "        conv = None\n",
        "        if use_conv_transpose:\n",
        "            conv = nn.ConvTranspose2d(channels, self.out_channels, 4, 2, 1)\n",
        "        elif use_conv:\n",
        "            conv = nn.Conv2d(self.channels, self.out_channels, 3, padding=1)\n",
        "\n",
        "        # TODO(Suraj, Patrick) - clean up after weight dicts are correctly renamed\n",
        "        if name == \"conv\":\n",
        "            self.conv = conv\n",
        "        else:\n",
        "            self.Conv2d_0 = conv\n",
        "\n",
        "    def forward(self, x):\n",
        "        assert x.shape[1] == self.channels\n",
        "        if self.use_conv_transpose:\n",
        "            return self.conv(x)\n",
        "\n",
        "        x = F.interpolate(x, scale_factor=2.0, mode=\"nearest\")\n",
        "\n",
        "        # TODO(Suraj, Patrick) - clean up after weight dicts are correctly renamed\n",
        "        if self.use_conv:\n",
        "            if self.name == \"conv\":\n",
        "                x = self.conv(x)\n",
        "            else:\n",
        "                x = self.Conv2d_0(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class AttentionBlockNew(nn.Module):\n",
        "    \"\"\"\n",
        "    An attention block that allows spatial positions to attend to each other. Originally ported from here, but adapted\n",
        "    to the N-d case.\n",
        "    https://github.com/hojonathanho/diffusion/blob/1e0dceb3b3495bbe19116a5e1b3596cd0706c543/diffusion_tf/models/unet.py#L66.\n",
        "    Uses three q, k, v linear layers to compute attention\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        channels,\n",
        "        num_head_channels=None,\n",
        "        num_groups=32,\n",
        "        rescale_output_factor=1.0,\n",
        "        eps=1e-5,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.channels = channels\n",
        "\n",
        "        self.num_heads = channels // num_head_channels if num_head_channels is not None else 1\n",
        "        self.num_head_size = num_head_channels\n",
        "        self.group_norm = nn.GroupNorm(num_channels=channels, num_groups=num_groups, eps=eps, affine=True)\n",
        "\n",
        "        # define q,k,v as linear layers\n",
        "        self.query = nn.Linear(channels, channels)\n",
        "        self.key = nn.Linear(channels, channels)\n",
        "        self.value = nn.Linear(channels, channels)\n",
        "\n",
        "        self.rescale_output_factor = rescale_output_factor\n",
        "        self.proj_attn = nn.Linear(channels, channels, 1)\n",
        "\n",
        "    def transpose_for_scores(self, projection: torch.Tensor) -> torch.Tensor:\n",
        "        new_projection_shape = projection.size()[:-1] + (self.num_heads, -1)\n",
        "        # move heads to 2nd position (B, T, H * D) -> (B, T, H, D) -> (B, H, T, D)\n",
        "        new_projection = projection.view(new_projection_shape).permute(0, 2, 1, 3)\n",
        "        return new_projection\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        residual = hidden_states\n",
        "        batch, channel, height, width = hidden_states.shape\n",
        "\n",
        "        # norm\n",
        "        hidden_states = self.group_norm(hidden_states)\n",
        "\n",
        "        hidden_states = hidden_states.view(batch, channel, height * width).transpose(1, 2)\n",
        "\n",
        "        # proj to q, k, v\n",
        "        query_proj = self.query(hidden_states)\n",
        "        key_proj = self.key(hidden_states)\n",
        "        value_proj = self.value(hidden_states)\n",
        "\n",
        "        # transpose\n",
        "        query_states = self.transpose_for_scores(query_proj)\n",
        "        key_states = self.transpose_for_scores(key_proj)\n",
        "        value_states = self.transpose_for_scores(value_proj)\n",
        "\n",
        "        # get scores\n",
        "        scale = 1 / math.sqrt(math.sqrt(self.channels / self.num_heads))\n",
        "        attention_scores = torch.matmul(query_states * scale, key_states.transpose(-1, -2) * scale)\n",
        "        attention_probs = torch.softmax(attention_scores.float(), dim=-1).type(attention_scores.dtype)\n",
        "\n",
        "        # compute attention output\n",
        "        context_states = torch.matmul(attention_probs, value_states)\n",
        "\n",
        "        context_states = context_states.permute(0, 2, 1, 3).contiguous()\n",
        "        new_context_states_shape = context_states.size()[:-2] + (self.channels,)\n",
        "        context_states = context_states.view(new_context_states_shape)\n",
        "\n",
        "        # compute next hidden_states\n",
        "        hidden_states = self.proj_attn(context_states)\n",
        "        hidden_states = hidden_states.transpose(-1, -2).reshape(batch, channel, height, width)\n",
        "\n",
        "        # res connect and rescale\n",
        "        hidden_states = (hidden_states + residual) / self.rescale_output_factor\n",
        "        return hidden_states\n",
        "\n",
        "    def set_weight(self, attn_layer):\n",
        "        self.group_norm.weight.data = attn_layer.norm.weight.data\n",
        "        self.group_norm.bias.data = attn_layer.norm.bias.data\n",
        "\n",
        "        if hasattr(attn_layer, \"q\"):\n",
        "            self.query.weight.data = attn_layer.q.weight.data[:, :, 0, 0]\n",
        "            self.key.weight.data = attn_layer.k.weight.data[:, :, 0, 0]\n",
        "            self.value.weight.data = attn_layer.v.weight.data[:, :, 0, 0]\n",
        "\n",
        "            self.query.bias.data = attn_layer.q.bias.data\n",
        "            self.key.bias.data = attn_layer.k.bias.data\n",
        "            self.value.bias.data = attn_layer.v.bias.data\n",
        "\n",
        "            self.proj_attn.weight.data = attn_layer.proj_out.weight.data[:, :, 0, 0]\n",
        "            self.proj_attn.bias.data = attn_layer.proj_out.bias.data\n",
        "        elif hasattr(attn_layer, \"NIN_0\"):\n",
        "            self.query.weight.data = attn_layer.NIN_0.W.data.T\n",
        "            self.key.weight.data = attn_layer.NIN_1.W.data.T\n",
        "            self.value.weight.data = attn_layer.NIN_2.W.data.T\n",
        "\n",
        "            self.query.bias.data = attn_layer.NIN_0.b.data\n",
        "            self.key.bias.data = attn_layer.NIN_1.b.data\n",
        "            self.value.bias.data = attn_layer.NIN_2.b.data\n",
        "\n",
        "            self.proj_attn.weight.data = attn_layer.NIN_3.W.data.T\n",
        "            self.proj_attn.bias.data = attn_layer.NIN_3.b.data\n",
        "\n",
        "            self.group_norm.weight.data = attn_layer.GroupNorm_0.weight.data\n",
        "            self.group_norm.bias.data = attn_layer.GroupNorm_0.bias.data\n",
        "        else:\n",
        "            qkv_weight = attn_layer.qkv.weight.data.reshape(\n",
        "                self.num_heads, 3 * self.channels // self.num_heads, self.channels\n",
        "            )\n",
        "            qkv_bias = attn_layer.qkv.bias.data.reshape(self.num_heads, 3 * self.channels // self.num_heads)\n",
        "\n",
        "            q_w, k_w, v_w = qkv_weight.split(self.channels // self.num_heads, dim=1)\n",
        "            q_b, k_b, v_b = qkv_bias.split(self.channels // self.num_heads, dim=1)\n",
        "\n",
        "            self.query.weight.data = q_w.reshape(-1, self.channels)\n",
        "            self.key.weight.data = k_w.reshape(-1, self.channels)\n",
        "            self.value.weight.data = v_w.reshape(-1, self.channels)\n",
        "\n",
        "            self.query.bias.data = q_b.reshape(-1)\n",
        "            self.key.bias.data = k_b.reshape(-1)\n",
        "            self.value.bias.data = v_b.reshape(-1)\n",
        "\n",
        "            self.proj_attn.weight.data = attn_layer.proj.weight.data[:, :, 0]\n",
        "            self.proj_attn.bias.data = attn_layer.proj.bias.data\n",
        "\n",
        "class UNetMidBlock2D(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels: int,\n",
        "        temb_channels: int,\n",
        "        dropout: float = 0.0,\n",
        "        num_layers: int = 1,\n",
        "        resnet_eps: float = 1e-6,\n",
        "        resnet_time_scale_shift: str = \"default\",\n",
        "        resnet_act_fn: str = \"swish\",\n",
        "        resnet_groups: int = 32,\n",
        "        resnet_pre_norm: bool = True,\n",
        "        attn_num_head_channels=1,\n",
        "        attention_type=\"default\",\n",
        "        output_scale_factor=1.0,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.attention_type = attention_type\n",
        "        resnet_groups = resnet_groups if resnet_groups is not None else min(in_channels // 4, 32)\n",
        "\n",
        "        # there is always at least one resnet\n",
        "        resnets = [\n",
        "            ResnetBlock(\n",
        "                in_channels=in_channels,\n",
        "                out_channels=in_channels,\n",
        "                temb_channels=temb_channels,\n",
        "                eps=resnet_eps,\n",
        "                groups=resnet_groups,\n",
        "                dropout=dropout,\n",
        "                time_embedding_norm=resnet_time_scale_shift,\n",
        "                non_linearity=resnet_act_fn,\n",
        "                output_scale_factor=output_scale_factor,\n",
        "                pre_norm=resnet_pre_norm,\n",
        "            )\n",
        "        ]\n",
        "        attentions = []\n",
        "\n",
        "        for _ in range(num_layers):\n",
        "            attentions.append(\n",
        "                AttentionBlockNew(\n",
        "                    in_channels,\n",
        "                    num_head_channels=attn_num_head_channels,\n",
        "                    rescale_output_factor=output_scale_factor,\n",
        "                    eps=resnet_eps,\n",
        "                    num_groups=resnet_groups,\n",
        "                )\n",
        "            )\n",
        "            resnets.append(\n",
        "                ResnetBlock(\n",
        "                    in_channels=in_channels,\n",
        "                    out_channels=in_channels,\n",
        "                    temb_channels=temb_channels,\n",
        "                    eps=resnet_eps,\n",
        "                    groups=resnet_groups,\n",
        "                    dropout=dropout,\n",
        "                    time_embedding_norm=resnet_time_scale_shift,\n",
        "                    non_linearity=resnet_act_fn,\n",
        "                    output_scale_factor=output_scale_factor,\n",
        "                    pre_norm=resnet_pre_norm,\n",
        "                )\n",
        "            )\n",
        "\n",
        "        self.attentions = nn.ModuleList(attentions)\n",
        "        self.resnets = nn.ModuleList(resnets)\n",
        "\n",
        "    def forward(self, hidden_states, temb=None, encoder_states=None):\n",
        "        hidden_states = self.resnets[0](hidden_states, temb)\n",
        "        for attn, resnet in zip(self.attentions, self.resnets[1:]):\n",
        "            if self.attention_type == \"default\":\n",
        "                hidden_states = attn(hidden_states)\n",
        "            else:\n",
        "                hidden_states = attn(hidden_states, encoder_states)\n",
        "            hidden_states = resnet(hidden_states, temb)\n",
        "\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels=3,\n",
        "        out_channels=4,\n",
        "        down_block_types=(\"DownEncoderBlock2D\",),\n",
        "        block_out_channels=(64,),\n",
        "        layers_per_block=1,\n",
        "        act_fn=\"silu\",\n",
        "        double_z=True,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.layers_per_block = layers_per_block\n",
        "        self.conv_in = torch.nn.Conv2d(in_channels, block_out_channels[0], kernel_size=3, stride=1, padding=1)\n",
        "        self.mid_block = None\n",
        "        self.down_blocks = nn.ModuleList([])\n",
        "\n",
        "        # down\n",
        "        output_channel = block_out_channels[0]\n",
        "        for i, _ in enumerate(down_block_types):\n",
        "            input_channel = output_channel\n",
        "            output_channel = block_out_channels[i]\n",
        "            is_final_block = i == len(block_out_channels) - 1\n",
        "\n",
        "            down_block = DownEncoderBlock2D(\n",
        "                num_layers=self.layers_per_block,\n",
        "                in_channels=input_channel,\n",
        "                out_channels=output_channel,\n",
        "                add_downsample=not is_final_block,\n",
        "                resnet_eps=1e-6,\n",
        "                resnet_act_fn=act_fn,\n",
        "                downsample_padding=0,)\n",
        "\n",
        "            self.down_blocks.append(down_block)\n",
        "\n",
        "        # mid\n",
        "        self.mid_block = UNetMidBlock2D(\n",
        "            in_channels=block_out_channels[-1],\n",
        "            resnet_eps=1e-6,\n",
        "            resnet_act_fn=act_fn,\n",
        "            output_scale_factor=1,\n",
        "            resnet_time_scale_shift=\"default\",\n",
        "            attn_num_head_channels=None,\n",
        "            resnet_groups=32,\n",
        "            temb_channels=None,\n",
        "        )\n",
        "\n",
        "        # out\n",
        "        num_groups_out = 32\n",
        "        self.conv_norm_out = nn.GroupNorm(num_channels=block_out_channels[-1], num_groups=num_groups_out, eps=1e-6)\n",
        "        self.conv_act = nn.SiLU()\n",
        "\n",
        "        conv_out_channels = 2 * out_channels if double_z else out_channels\n",
        "        self.conv_out = nn.Conv2d(block_out_channels[-1], conv_out_channels, 3, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \n",
        "        sample = x\n",
        "        sample = self.conv_in(sample)\n",
        "        \n",
        "        # down\n",
        "        for down_block in self.down_blocks:\n",
        "            sample = down_block(sample)\n",
        "\n",
        "        # middle\n",
        "        sample = self.mid_block(sample)\n",
        "\n",
        "        # post-process\n",
        "        sample = self.conv_norm_out(sample)\n",
        "        sample = self.conv_act(sample)\n",
        "        sample = self.conv_out(sample)\n",
        "\n",
        "        return sample\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels=3,\n",
        "        out_channels=3,\n",
        "        up_block_types=(\"UpDecoderBlock2D\",),\n",
        "        block_out_channels=(64,),\n",
        "        layers_per_block=2,\n",
        "        act_fn=\"silu\",\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.layers_per_block = layers_per_block\n",
        "\n",
        "        self.conv_in = nn.Conv2d(in_channels, block_out_channels[-1], kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "        self.mid_block = None\n",
        "        self.up_blocks = nn.ModuleList([])\n",
        "\n",
        "        # mid\n",
        "        self.mid_block = UNetMidBlock2D(\n",
        "            in_channels=block_out_channels[-1],\n",
        "            resnet_eps=1e-6,\n",
        "            resnet_act_fn=act_fn,\n",
        "            output_scale_factor=1,\n",
        "            resnet_time_scale_shift=\"default\",\n",
        "            attn_num_head_channels=None,\n",
        "            resnet_groups=32,\n",
        "            temb_channels=None,\n",
        "        )\n",
        "\n",
        "        # up\n",
        "        reversed_block_out_channels = list(reversed(block_out_channels))\n",
        "        output_channel = reversed_block_out_channels[0]\n",
        "        for i, up_block_type in enumerate(up_block_types):\n",
        "            prev_output_channel = output_channel\n",
        "            output_channel = reversed_block_out_channels[i]\n",
        "\n",
        "            is_final_block = i == len(block_out_channels) - 1\n",
        "\n",
        "            up_block = UpDecoderBlock2D(\n",
        "            num_layers=self.layers_per_block + 1,\n",
        "            in_channels=prev_output_channel,\n",
        "            out_channels=output_channel,\n",
        "            add_upsample=not is_final_block,\n",
        "            resnet_eps=1e-6,\n",
        "            resnet_act_fn=act_fn,\n",
        "        )\n",
        "\n",
        "            self.up_blocks.append(up_block)\n",
        "            prev_output_channel = output_channel\n",
        "\n",
        "        # out\n",
        "        num_groups_out = 32\n",
        "        self.conv_norm_out = nn.GroupNorm(num_channels=block_out_channels[0], num_groups=num_groups_out, eps=1e-6)\n",
        "        self.conv_act = nn.SiLU()\n",
        "        self.conv_out = nn.Conv2d(block_out_channels[0], out_channels, 3, padding=1)\n",
        "\n",
        "    def forward(self, z):\n",
        "        sample = z\n",
        "        sample = self.conv_in(sample)\n",
        "\n",
        "        # middle\n",
        "        sample = self.mid_block(sample)\n",
        "\n",
        "        # up\n",
        "        for up_block in self.up_blocks:\n",
        "            sample = up_block(sample)\n",
        "\n",
        "        # post-process\n",
        "        sample = self.conv_norm_out(sample)\n",
        "        sample = self.conv_act(sample)\n",
        "        sample = self.conv_out(sample)\n",
        "\n",
        "        return sample"
      ],
      "metadata": {
        "id": "5_K2AE--WiX4"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DiagonalGaussianDistribution(object):\n",
        "    def __init__(self, parameters, deterministic=False):\n",
        "        self.parameters = parameters\n",
        "        self.mean, self.logvar = torch.chunk(parameters, 2, dim=1)\n",
        "        self.logvar = torch.clamp(self.logvar, -30.0, 20.0)\n",
        "        self.deterministic = deterministic\n",
        "        self.std = torch.exp(0.5 * self.logvar)\n",
        "        self.var = torch.exp(self.logvar)\n",
        "        if self.deterministic:\n",
        "            self.var = self.std = torch.zeros_like(self.mean).to(device=self.parameters.device)\n",
        "\n",
        "    def sample(self):\n",
        "        x = self.mean + self.std * torch.randn(self.mean.shape).to(device=self.parameters.device)\n",
        "        return x\n",
        "\n",
        "    def kl(self, other=None):\n",
        "        if self.deterministic:\n",
        "            return torch.Tensor([0.0])\n",
        "        else:\n",
        "            if other is None:\n",
        "                return 0.5 * torch.sum(torch.pow(self.mean, 2) + self.var - 1.0 - self.logvar, dim=[1, 2, 3])\n",
        "            else:\n",
        "                return 0.5 * torch.sum(\n",
        "                    torch.pow(self.mean - other.mean, 2) / other.var\n",
        "                    + self.var / other.var\n",
        "                    - 1.0\n",
        "                    - self.logvar\n",
        "                    + other.logvar,\n",
        "                    dim=[1, 2, 3],\n",
        "                )\n",
        "\n",
        "    def nll(self, sample, dims=[1, 2, 3]):\n",
        "        if self.deterministic:\n",
        "            return torch.Tensor([0.0])\n",
        "        logtwopi = np.log(2.0 * np.pi)\n",
        "        return 0.5 * torch.sum(logtwopi + self.logvar + torch.pow(sample - self.mean, 2) / self.var, dim=dims)\n",
        "\n",
        "    def mode(self):\n",
        "        return self.mean\n",
        "\n",
        "\n",
        "\n",
        "encoder = Encoder(in_channels=3,\n",
        "        out_channels=4,\n",
        "        down_block_types=(\"DownEncoderBlock2D\",),\n",
        "        block_out_channels=(64,),\n",
        "        layers_per_block=1,\n",
        "        act_fn=\"silu\",\n",
        "        double_z=True)\n",
        "\n",
        "\n",
        "decoder = Decoder(in_channels=4,\n",
        "            out_channels=3,\n",
        "            up_block_types=(\"UpDecoderBlock2D\",),\n",
        "            block_out_channels=(64,),\n",
        "            layers_per_block=1,\n",
        "            act_fn=\"silu\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    ##########################\n",
        "    ##########################\n",
        "    ##########################\n",
        "#### AutoEncoder wrap- up class ####\n",
        "    ##########################\n",
        "    ##########################\n",
        "    ##########################\n",
        "\n",
        "class AutoencoderKL(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels=3,\n",
        "        out_channels=3,\n",
        "        down_block_types=(\"DownEncoderBlock2D\",\"DownEncoderBlock2D\",\"DownEncoderBlock2D\",\"DownEncoderBlock2D\",),\n",
        "        up_block_types=(\"UpDecoderBlock2D\",\"UpDecoderBlock2D\",\"UpDecoderBlock2D\",\"UpDecoderBlock2D\",),\n",
        "        block_out_channels=(128, 256, 512, 512,),\n",
        "        layers_per_block=2,\n",
        "        act_fn=\"silu\",\n",
        "        latent_channels=4,\n",
        "        sample_size=512,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        # pass init params to Encoder\n",
        "        self.encoder = Encoder(\n",
        "            in_channels=in_channels,\n",
        "            out_channels=latent_channels,\n",
        "            down_block_types=down_block_types,\n",
        "            block_out_channels=block_out_channels,\n",
        "            layers_per_block=layers_per_block,\n",
        "            act_fn=act_fn,\n",
        "            double_z=True,\n",
        "        )\n",
        "\n",
        "        # pass init params to Decoder\n",
        "        self.decoder = Decoder(\n",
        "            in_channels=latent_channels,\n",
        "            out_channels=out_channels,\n",
        "            up_block_types=up_block_types,\n",
        "            block_out_channels=block_out_channels,\n",
        "            layers_per_block=layers_per_block,\n",
        "            act_fn=act_fn,\n",
        "        )\n",
        "\n",
        "        self.quant_conv = torch.nn.Conv2d(2 * latent_channels, 2 * latent_channels, 1)\n",
        "        self.post_quant_conv = torch.nn.Conv2d(latent_channels, latent_channels, 1) \n",
        "        self.kl = 0\n",
        "\n",
        "    def encode(self, x):\n",
        "        h = self.encoder(x)\n",
        "        moments = self.quant_conv(h)\n",
        "        posterior = DiagonalGaussianDistribution(moments)\n",
        "        return posterior\n",
        "\n",
        "    def decode(self, z):\n",
        "        z = self.post_quant_conv(z)\n",
        "        dec = self.decoder(z)\n",
        "        return dec\n",
        "\n",
        "    def forward(self, sample, sample_posterior=False):\n",
        "        x = sample\n",
        "        posterior = self.encode(x)\n",
        "        if sample_posterior:\n",
        "            z = posterior.sample()\n",
        "        else:\n",
        "            z = posterior.mode()\n",
        "        dec = self.decode(z)\n",
        "        self.kl=posterior.kl()\n",
        "        return dec\n"
      ],
      "metadata": {
        "id": "wpwvB6BhWvDF"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Training function**"
      ],
      "metadata": {
        "id": "crEhgj4FVZTB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##### train\n",
        "from tqdm.auto import tqdm\n",
        "PATH = './vae_net.pth' # default path\n",
        "losses=[]\n",
        "\n",
        "def train(autoencoder, data, epochs=20,exp=None,lr=0.001,schedualer_step=1000):\n",
        "    \n",
        "    def adjust(sample):\n",
        "        # sample=np.expand_dims(np.asarray(sample), axis=0)\n",
        "        sample=sample.numpy()/ 1.0\n",
        "        # sample=sample.astype('float32')\n",
        "        sample = torch.from_numpy(sample).float() # Batch - RGB channel - WxH \n",
        "        sample = 2 * (sample - 0.5) # values between (-1, 1)\n",
        "        return sample\n",
        "      \n",
        "    def variational_loss(x,x_hat):\n",
        "        return F.mse_loss(x,x_hat) + torch.sum(autoencoder.kl,dim=0)\n",
        "\n",
        "    if exp:\n",
        "        model_file='./{}_net.pth'.format(exp)\n",
        "    else:\n",
        "        model_file=PATH\n",
        "    \n",
        "    global opt\n",
        "    opt = torch.optim.Adam(autoencoder.parameters(),lr=lr)\n",
        "    scheduler = torch.optim.lr_scheduler.ExponentialLR(opt, gamma=0.9,verbose=True)\n",
        "    epoch = 0\n",
        "\n",
        "    if os.path.exists(model_file):\n",
        "      checkpoint = torch.load(model_file)\n",
        "      autoencoder.load_state_dict(checkpoint['model_state_dict'])\n",
        "      opt.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "      scheduler.load_state_dict(checkpoint['scheduler'])\n",
        "      epoch = checkpoint['epoch']\n",
        "      loss = checkpoint['loss']\n",
        "\n",
        "    for epoch in range(epoch, epochs):\n",
        "        # print(\"Log epoch:\",epoch)\n",
        "\n",
        "       with tqdm(data, unit=\"batch\",total=len(data)) as tepoch:\n",
        "        for i, x in enumerate(tepoch):\n",
        "            tepoch.set_description(f\"Epoch {epoch}\")\n",
        "\n",
        "        # for i,x in enumerate(tqdm(data,total=len(data))):\n",
        "            x=adjust(x[0]) # Values between -1..1 \n",
        "            x = torch.Tensor(x).to(device) # GPU\n",
        "            opt.zero_grad()\n",
        "            x_hat = autoencoder(x,sample_posterior=True)\n",
        "            loss = ((x - x_hat)**2).sum() + autoencoder.kl.sum()\n",
        "            # loss = variational_loss(x,x_hat)\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            loss=loss.item()\n",
        "            losses.append(loss)\n",
        "            tepoch.set_postfix(loss=loss)\n",
        "\n",
        "            # print(loss)\n",
        "\n",
        "            if not i % schedualer_step and i != 0: \n",
        "                torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': autoencoder.state_dict(),\n",
        "                'optimizer_state_dict': opt.state_dict(),\n",
        "                'loss': loss,\n",
        "                'scheduler':scheduler.state_dict()}, model_file)\n",
        "                \n",
        "                print(\"model saved!\")\n",
        "                scheduler.step()\n",
        "        \n",
        "        torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': autoencoder.state_dict(),\n",
        "                'optimizer_state_dict': opt.state_dict(),\n",
        "                'loss': loss,\n",
        "                'scheduler':scheduler.state_dict()}, model_file)\n",
        "        print(\"model saved!\")\n",
        "   \n",
        "    return autoencoder"
      ],
      "metadata": {
        "id": "hieXAg7cVX-x"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**DataLoader Cifar10** "
      ],
      "metadata": {
        "id": "AKgZYAGTVpnx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ToDo: https://gist.github.com/kevinzakka/d33bf8d6c7f06a9d8c76d97a7879f5cb\n",
        "# normalize dataset.\n",
        "\n",
        "def load_data(BS=32,path = \"./cifar-10-batches-py\",num_workers=4):\n",
        "    train = torch.utils.data.DataLoader(\n",
        "        torchvision.datasets.CIFAR10(path,\n",
        "               transform=torchvision.transforms.ToTensor(),\n",
        "               download=True),\n",
        "        batch_size=BS,\n",
        "        shuffle=True,\n",
        "        num_workers=num_workers)\n",
        "    \n",
        "    test = torch.utils.data.DataLoader(\n",
        "        torchvision.datasets.CIFAR10(path,\n",
        "               transform=torchvision.transforms.ToTensor(),\n",
        "               download=True,train=False),\n",
        "        batch_size=BS,\n",
        "        shuffle=True,\n",
        "        num_workers=num_workers)\n",
        "        \n",
        "    print(train.dataset)\n",
        "    print(test.dataset)\n",
        "    print(\"train data size:\",next(iter(train))[0].size())\n",
        "\n",
        "    return train,test\n"
      ],
      "metadata": {
        "id": "0ZSbEP8ik-CG"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train/Test Split**\n"
      ],
      "metadata": {
        "id": "dVGV1ZUMj7Fg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training Variational AutoEncoder on Cifar10 data."
      ],
      "metadata": {
        "id": "KWrWRwtFA6hh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data,test_data=load_data(BS=64,num_workers=4)\n",
        "vae=AutoencoderKL().to(device)\n",
        "vae = train(vae, train_data,epochs=10,exp=\"test5\",lr=1e-3,schedualer_step=1000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425,
          "referenced_widgets": [
            "d1d3b09cf1064675923821e8dadba729",
            "cdc5905f2a054a1f8ff868632eea4c78",
            "6206aabf38474507bc92d99872796328",
            "18eb6c034dd1477e83acdbaa73330b40",
            "6902e8f3d6b2447586223271a8e1cfd0",
            "55f26aa3a71549d7bea921f6349c0b9d",
            "7198543a611442358e41b29263ae2c85",
            "d5b74588877949dcb645e0524c811fec",
            "c2c3ab66678440b089b60143155ead76",
            "5f01630f1e174d2ba8acfd3c97e7287b",
            "915d687e2c124ae5837e148b462fd2f7",
            "54f73c0eeb114a21885f498ecbad5457",
            "ca5765510cdd44e19df43f086c7eba1a",
            "fd48d1c48e9e47b9a00c58c5666d6fa0",
            "973eff8b283648508c85b40fec4c118e",
            "c8b51952803f4b76b580504c85c7386f",
            "d546cba5c5304ecb98ac59314bddb267",
            "45d4cbf710724c9f8f8b6066519cb345",
            "247416a20a464edaa08092204fc645ca",
            "1319a5e7e8534d8ba1267a34fd895a31",
            "cfedb7ecee9b49a38626f559f11494a5",
            "a77b412e6c7f4a7f9bcf2bedf0265c6a",
            "9b82f1b604fc45ff9a83da64d4f32672",
            "faf85789146943e28adc90a996591227",
            "645f94885bdf4ab9998022b04df33c3b",
            "114dfa6aedd045f98d46293c955dfd5c",
            "3c10374eaa6a4f79b3a9a01f0bcf4451",
            "4a71e0058b7e4827a2c08550f27a45ed",
            "0b0ba60c97cc49fa862500dc2e1d5d90",
            "5ed195c423f04962831c74a84886500f",
            "cd024b6e2c124b48bc3c25093b27f6bd",
            "e0a366d23d1646e18d4ca9a8a9c6d139",
            "6aa4a8d0e22a475d86d5c6a550e23fbb"
          ]
        },
        "id": "sPWD7vMck51E",
        "outputId": "395a3ac5-b328-44ba-e0c8-c00a7299e989"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Dataset CIFAR10\n",
            "    Number of datapoints: 50000\n",
            "    Root location: ./cifar-10-batches-py\n",
            "    Split: Train\n",
            "    StandardTransform\n",
            "Transform: ToTensor()\n",
            "Dataset CIFAR10\n",
            "    Number of datapoints: 10000\n",
            "    Root location: ./cifar-10-batches-py\n",
            "    Split: Test\n",
            "    StandardTransform\n",
            "Transform: ToTensor()\n",
            "train data size: torch.Size([64, 3, 32, 32])\n",
            "Adjusting learning rate of group 0 to 1.0000e-03.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/782 [00:00<?, ?batch/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d1d3b09cf1064675923821e8dadba729"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model saved!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/782 [00:00<?, ?batch/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "54f73c0eeb114a21885f498ecbad5457"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model saved!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/782 [00:00<?, ?batch/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9b82f1b604fc45ff9a83da64d4f32672"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Check the results"
      ],
      "metadata": {
        "id": "CMxI7UsrTkVe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib.pyplot import plot\n",
        "\n",
        "plot(losses)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "E2aDBAF-ATeR",
        "outputId": "cbb45b15-08fc-4d54-af90-8e3d6092ddde"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f0770230310>]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib.pyplot import imshow\n",
        "%matplotlib inline\n",
        "def decode_img(img):\n",
        "    from PIL import Image\n",
        "    img = (img / 2 + 0.5).clamp(0, 1)\n",
        "    img = img.detach().cpu().permute(1, 2, 0).numpy()\n",
        "    img = (img * 255).round().astype('uint8')\n",
        "    pil_image = Image.fromarray(img)\n",
        "    return pil_image\n",
        "\n",
        "def adjust(sample):\n",
        "        # sample=np.expand_dims(np.asarray(sample), axis=0)\n",
        "        sample=sample.numpy()/ 1.0\n",
        "        # sample=sample.astype('float32')\n",
        "        sample = torch.from_numpy(sample).float() # Batch - RGB channel - WxH \n",
        "        sample = 2 * (sample - 0.5) # values between (-1, 1)\n",
        "        return sample\n"
      ],
      "metadata": {
        "id": "8XpNYhETpyvG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nu88VDINCZaJ",
        "outputId": "24344abb-0683-4d5e-ece4-9f16579d244c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cifar-10-batches-py  gdrive  sample_data  test2_net.pth  test3_net.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "exp=\"test2\"\n",
        "checkpoint = torch.load(f\"./{exp}_net.pth\")\n",
        "vae.load_state_dict(checkpoint['model_state_dict'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zdp_u-z8mbzC",
        "outputId": "3f8bf5a7-9369-4714-83c4-caf0e20f6966"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_imgs = next(iter(test_data))[0]\n",
        "img=batch_imgs[1]"
      ],
      "metadata": {
        "id": "WNF-e-xiC-2P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38959eda-cfe8-4b5d-913e-8fe477c8db15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:566: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decode_img(img)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49
        },
        "id": "f_A8abVtqv0j",
        "outputId": "d6e00f98-ac4a-4a1c-dd82-d211fef8bd91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=32x32 at 0x7F7BC66BCB50>"
            ],
            "image/png": "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\n"
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output=vae(adjust(torch.Tensor(img)[None, :]).to(device))[0]"
      ],
      "metadata": {
        "id": "gk7rLY7FqzbZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decode_img(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49
        },
        "id": "1Ykj1bp-oDTY",
        "outputId": "84d931f1-af91-4ca8-86c4-7395434f27bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=32x32 at 0x7F7BC675D410>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAIAAAD8GO2jAAADNElEQVR4nG2W0ZLbOgxDcRj99p0+3a9ul+gDSUl26tlxvIksEiAIil////fhYwuQBAASAYIgRD/3Ap3LsiTZnls6ZWfasmz/ZvHj1G+BCMAKQEhI82Bmw8f+Fc4WkkGyqGdZzswfRy4iJEWEIiICgoiNo7LWTr1zviP0ksrgY8smUs6fn8j8s6ioBAoUUnDlXMiRXQC+Lr8xCRWGkBKxiveA4kZ4v2qnJ0bnXiQdqt67V7b1PsIVwB6GJ+v+zr3rpM/Z9InGpyIV3RLeASpbKSWUs3onf/ZCYiByFnGtwRukJKdWBXWmwDTb8/vo8Ox+Q7mj7LQarUFSymsYTsyV++FHlgUnzPu6I3pEZxDYXpJtS2GbC4El6pcrsWsvvoJc0hMeVS/Zsu2UZN8FtAvEVenz4xcUV6+wmwYFspZlOy1kjLewC8Euw0Osr6tfwNOPgB21dMntILWWw0LbCZZlBk27ww1lg4Fxl1CkHBbLSjtVmyCb0DtG8+Xz74N8S+wWqb+0Axu6BtOxlkQK1Iq91FEiOvZ50cMJpLEnWiin0YZtUDWEEFeMRztxysGlKwS71UGqPnCV2c7yWpAV4GZ0yGC6lrPx3eaXrTbMMjvbaWfVU60jTL0ScFGwX9br6sFQBifj6Host06zQByZ7rnW9+GCY7hiB21TT40hG5tPfJa6lyynnN3Ip16xmd0jc9eF7m36nTYCu+UuyevU2M4G0R5qEeSezzWa2Gi+PMOeidlSCjsaQfEkp50VLSfGpmufCi7x3N8ckQpwRMhRCKaAnmFj72a1J6eqADVNXmH2AqAOItFWuTryNHlPoylbbX6Q7651fXuPUA+Nph2HarRxEAIsxVRsa+PW4vXsandV30DZ5Rx15nP1kYrooiqlUFSpWxbH5y/Vf0+Eouhaj6UlJAVkHY5kSkyjwXtEsD9PpKOpUbPOMVDSakUdD7WiRrdo/3zK4MHWs8hNdlSd61p1KHIEzaZdR799mLkGjW8IuxfY945BxL4XgohAjujx6e3dXfB/I9hUvwIIwgCBWAARPhbxyv1++Lp4xjhclaxQsGKhP3FV7Z87fke4NHTb9CCro5xSfwF/rWKB+IoV8wAAAABJRU5ErkJggg==\n"
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jeb2mkyhm-1B",
        "outputId": "3f6ef0ae-2f0e-41ae-fe26-7c52e7804769"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cifar-10-batches-py  gdrive  sample_data  test2_net.pth  test3_net.pth\n"
          ]
        }
      ]
    }
  ]
}