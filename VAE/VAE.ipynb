{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMYrhafm3g6ABQPyeyVmx6h",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1dd545a4065a453bb0522b6dad369d18": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cbb3e814172749d89ba02aa02fb1d2fa",
              "IPY_MODEL_3f00a014436e4535b8921b1620878416",
              "IPY_MODEL_47a2b54350664393889abc818011cfd8"
            ],
            "layout": "IPY_MODEL_b06b4a5cea214634ad745a3d78cac170"
          }
        },
        "cbb3e814172749d89ba02aa02fb1d2fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2a5402e930e2420486c4ed62b61e8d3c",
            "placeholder": "​",
            "style": "IPY_MODEL_2f3367f5cc5e4cd6931c377d62673965",
            "value": "Epoch 0:  66%"
          }
        },
        "3f00a014436e4535b8921b1620878416": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_daaaefe5ee714b40aa3d277b9223c365",
            "max": 1563,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_490590f21ac341298e02e63a6a12c822",
            "value": 1030
          }
        },
        "47a2b54350664393889abc818011cfd8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_341a860811a543af9a8e3d95b7b97c0d",
            "placeholder": "​",
            "style": "IPY_MODEL_a570655de1f44464a31de57d7e4e8a3e",
            "value": " 1030/1563 [06:57&lt;02:43,  3.25batch/s, loss=0.275]"
          }
        },
        "b06b4a5cea214634ad745a3d78cac170": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2a5402e930e2420486c4ed62b61e8d3c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2f3367f5cc5e4cd6931c377d62673965": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "daaaefe5ee714b40aa3d277b9223c365": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "490590f21ac341298e02e63a6a12c822": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "341a860811a543af9a8e3d95b7b97c0d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a570655de1f44464a31de57d7e4e8a3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cohitai/AI_notebooks/blob/main/VAE/VAE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Variational AutoEncoder = (encoder= Q(Z|X)=Recognition model, decoder=P(X|Z)=Generative model)+ Variational loss.\n",
        "\n",
        " Variational loss = ELBO (evidence lower bound). \n",
        "\n"
      ],
      "metadata": {
        "id": "TjIgUpV0H-Nq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ToDO: visualizations of the latent space, KL divergence explanations, Generative data. \n",
        "\n",
        "# Go over these: \n",
        "\n",
        "# https://avandekleut.github.io/vae/\n",
        "\n",
        "# https://github.com/AntixK/PyTorch-VAE (many different types of VAEs)\n",
        "# https://www.jeremyjordan.me/variational-autoencoders/\n",
        "# https://gaussian37.github.io/deep-learning-chollet-8-4/\n",
        "# https://medium.com/@arjun.majumdar/variational-autoencoder-cifar-10-tf2-9ed1155771e1\n",
        "# https://github.com/arjun-majumdar/Autoencoders_Experiments/blob/master/Variational_AutoEncoder_CIFAR10_TF2.ipynb\n",
        "# https://towardsdatascience.com/variational-autoencoder-demystified-with-pytorch-implementation-3a06bee395ed\n",
        "# https://colab.research.google.com/drive/1_yGmk8ahWhDs23U4mpplBFa-39fsEJoT?usp=sharing#scrollTo=xOVursP7lpR-\n",
        "\n",
        "# More ToDo's:\n",
        "# 1. plot the losses.\n",
        "# 2. plot reconstruction versus original images as 2 X M subplot. \n",
        "# 2. Visualization of the latent space from https://avandekleut.github.io/vae/\n",
        "# 3. evaluation data + some kind of accuracy\n",
        "# 4. sum() or mean() in loss , and kl.sum() (probabely) or kl.mean() ?  (stochstic !)\n",
        "# 5. Compare the loss with the pretrained VAE model(Huggingfaces). \n",
        "\n",
        "# Improvements\n",
        "# 0. adjust the learning rate better\n",
        "# 1. Exponential Learning rate ? shadow variables etc \n",
        "# 2. Perceptual Losses for Deep Image Restoration\n",
        "# 3. https://towardsdatascience.com/perceptual-losses-for-image-restoration-dd3c9de4113\n",
        "# 4. Compare the loss with the pretrained VAE model(Huggingfaces). \n",
        "\n",
        "\n",
        "# Theory: main paper on VAE: (Auto-Encoding Variational Bayes) at https://arxiv.org/pdf/1312.6114.pdf\n",
        "\n",
        "# Unet variations\n",
        "# Unet: https://amaarora.github.io/2020/09/13/unet.html\n",
        "# Unet paper: U-Net: Convolutional Networks for Biomedical Image Segmentation (https://arxiv.org/pdf/1505.04597.pdf)\n",
        "\n",
        "# Diffusers on local M1 ANE (transfer to Colab). "
      ],
      "metadata": {
        "id": "VrgX3VIFo8Dk"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch \n",
        "import torchvision\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import os\n",
        "import math\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "print(\"Working directory:\", os.getcwd())\n",
        "device=\"cuda\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rMwmbbQMWjkm",
        "outputId": "7e5ca64c-f309-4085-9c83-e914733b9bdb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "Working directory: /content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d-ZChfoGGKpW",
        "outputId": "c1995195-0234-4613-bf9c-e3f39f43064e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri Nov 18 22:58:00 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   69C    P0    30W /  70W |  10034MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Blocks:\n",
        " - DownEncoderBlock2D\n",
        "\n",
        " - UpDecoderBlock2D\n",
        "\n",
        " - ResnetBlock\n",
        "\n",
        " - Downsample2D\n",
        "\n",
        " - Upsample2D\n",
        "\n",
        " - AttentionBlockNew\n",
        "\n",
        " - UNetMidBlock2D\n",
        "\n",
        " - Encoder\n",
        "\n",
        " - Decoder\n"
      ],
      "metadata": {
        "id": "JZ3V7x4oWwCJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DownEncoderBlock2D(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels: int,\n",
        "        out_channels: int,\n",
        "        dropout: float = 0.0, \n",
        "        num_layers: int = 1,\n",
        "        resnet_eps: float = 1e-6,\n",
        "        resnet_time_scale_shift: str = \"default\",\n",
        "        resnet_act_fn: str = \"swish\",\n",
        "        resnet_groups: int = 32,\n",
        "        resnet_pre_norm: bool = True,\n",
        "        output_scale_factor=1.0,\n",
        "        add_downsample=True,\n",
        "        downsample_padding=1,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        resnets = []\n",
        "\n",
        "        for i in range(num_layers):\n",
        "            in_channels = in_channels if i == 0 else out_channels\n",
        "            resnets.append(\n",
        "                ResnetBlock(\n",
        "                    in_channels=in_channels,\n",
        "                    out_channels=out_channels,\n",
        "                    temb_channels=None,\n",
        "                    eps=resnet_eps,\n",
        "                    groups=resnet_groups,\n",
        "                    dropout=dropout,\n",
        "                    time_embedding_norm=resnet_time_scale_shift,\n",
        "                    non_linearity=resnet_act_fn,\n",
        "                    output_scale_factor=output_scale_factor,\n",
        "                    pre_norm=resnet_pre_norm,\n",
        "                )\n",
        "            )\n",
        "\n",
        "        self.resnets = nn.ModuleList(resnets)\n",
        "\n",
        "        if add_downsample:\n",
        "            self.downsamplers = nn.ModuleList(\n",
        "                [\n",
        "                    Downsample2D(\n",
        "                        in_channels, use_conv=True, out_channels=out_channels, padding=downsample_padding, name=\"op\"\n",
        "                    )\n",
        "                ]\n",
        "            )\n",
        "        else:\n",
        "            self.downsamplers = None\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        for resnet in self.resnets:\n",
        "            hidden_states = resnet(hidden_states, temb=None)\n",
        "\n",
        "        if self.downsamplers is not None:\n",
        "            for downsampler in self.downsamplers:\n",
        "                hidden_states = downsampler(hidden_states)\n",
        "\n",
        "        return hidden_states\n",
        "\n",
        "class UpDecoderBlock2D(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels: int,\n",
        "        out_channels: int,\n",
        "        dropout: float = 0.0,\n",
        "        num_layers: int = 1,\n",
        "        resnet_eps: float = 1e-6,\n",
        "        resnet_time_scale_shift: str = \"default\",\n",
        "        resnet_act_fn: str = \"swish\",\n",
        "        resnet_groups: int = 32,\n",
        "        resnet_pre_norm: bool = True,\n",
        "        output_scale_factor=1.0,\n",
        "        add_upsample=True,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        resnets = []\n",
        "\n",
        "        for i in range(num_layers):\n",
        "            input_channels = in_channels if i == 0 else out_channels\n",
        "\n",
        "            resnets.append(\n",
        "                ResnetBlock(\n",
        "                    in_channels=input_channels,\n",
        "                    out_channels=out_channels,\n",
        "                    temb_channels=None,\n",
        "                    eps=resnet_eps,\n",
        "                    groups=resnet_groups,\n",
        "                    dropout=dropout,\n",
        "                    time_embedding_norm=resnet_time_scale_shift,\n",
        "                    non_linearity=resnet_act_fn,\n",
        "                    output_scale_factor=output_scale_factor,\n",
        "                    pre_norm=resnet_pre_norm,\n",
        "                )\n",
        "            )\n",
        "\n",
        "        self.resnets = nn.ModuleList(resnets)\n",
        "\n",
        "        if add_upsample:\n",
        "            self.upsamplers = nn.ModuleList([Upsample2D(out_channels, use_conv=True, out_channels=out_channels)])\n",
        "        else:\n",
        "            self.upsamplers = None\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        for resnet in self.resnets:\n",
        "            hidden_states = resnet(hidden_states, temb=None)\n",
        "\n",
        "        if self.upsamplers is not None:\n",
        "            for upsampler in self.upsamplers:\n",
        "                hidden_states = upsampler(hidden_states)\n",
        "\n",
        "        return hidden_states\n",
        "\n",
        "  \n",
        "    \n",
        "class ResnetBlock(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        *,\n",
        "        in_channels,\n",
        "        out_channels=None,\n",
        "        conv_shortcut=False,\n",
        "        dropout=0.0,\n",
        "        temb_channels=512,\n",
        "        groups=32,\n",
        "        groups_out=None,\n",
        "        pre_norm=True,\n",
        "        eps=1e-6,\n",
        "        non_linearity=\"swish\",\n",
        "        time_embedding_norm=\"default\",\n",
        "        kernel=None,\n",
        "        output_scale_factor=1.0,\n",
        "        use_nin_shortcut=None,\n",
        "        up=False,\n",
        "        down=False,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.pre_norm = pre_norm\n",
        "        self.pre_norm = True\n",
        "        self.in_channels = in_channels\n",
        "        out_channels = in_channels if out_channels is None else out_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.use_conv_shortcut = conv_shortcut\n",
        "        self.time_embedding_norm = time_embedding_norm\n",
        "        self.up = up\n",
        "        self.down = down\n",
        "        self.output_scale_factor = output_scale_factor\n",
        "\n",
        "        if groups_out is None:\n",
        "            groups_out = groups\n",
        "\n",
        "        self.norm1 = torch.nn.GroupNorm(num_groups=groups, num_channels=in_channels, eps=eps, affine=True)\n",
        "\n",
        "        self.conv1 = torch.nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "        if temb_channels is not None:\n",
        "            self.time_emb_proj = torch.nn.Linear(temb_channels, out_channels)\n",
        "        else:\n",
        "            self.time_emb_proj = None\n",
        "\n",
        "        self.norm2 = torch.nn.GroupNorm(num_groups=groups_out, num_channels=out_channels, eps=eps, affine=True)\n",
        "        self.dropout = torch.nn.Dropout(dropout)\n",
        "        self.conv2 = torch.nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "        if non_linearity == \"swish\":\n",
        "            self.nonlinearity = lambda x: F.silu(x)\n",
        "        elif non_linearity == \"mish\":\n",
        "            self.nonlinearity = Mish()\n",
        "        elif non_linearity == \"silu\":\n",
        "            self.nonlinearity = nn.SiLU()\n",
        "\n",
        "        self.upsample = self.downsample = None\n",
        "        if self.up:\n",
        "            if kernel == \"fir\":\n",
        "                fir_kernel = (1, 3, 3, 1)\n",
        "                self.upsample = lambda x: upsample_2d(x, k=fir_kernel)\n",
        "            elif kernel == \"sde_vp\":\n",
        "                self.upsample = partial(F.interpolate, scale_factor=2.0, mode=\"nearest\")\n",
        "            else:\n",
        "                self.upsample = Upsample2D(in_channels, use_conv=False)\n",
        "        \n",
        "        elif self.down:\n",
        "            if kernel == \"fir\":\n",
        "                fir_kernel = (1, 3, 3, 1)\n",
        "                self.downsample = lambda x: downsample_2d(x, k=fir_kernel)\n",
        "            elif kernel == \"sde_vp\":\n",
        "                self.downsample = partial(F.avg_pool2d, kernel_size=2, stride=2)\n",
        "            else:\n",
        "                self.downsample = Downsample2D(in_channels, use_conv=False, padding=1, name=\"op\")\n",
        "\n",
        "        self.use_nin_shortcut = self.in_channels != self.out_channels if use_nin_shortcut is None else use_nin_shortcut\n",
        "\n",
        "        self.conv_shortcut = None\n",
        "    \n",
        "        if self.use_nin_shortcut:\n",
        "            self.conv_shortcut = torch.nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0)\n",
        "\n",
        "    def forward(self, x, temb, hey=False):\n",
        "        h = x\n",
        "\n",
        "        # print(\"LOG resnet_input\",h.size())\n",
        "\n",
        "        # make sure hidden states is in float32\n",
        "        # when running in half-precision\n",
        "        h = self.norm1(h.float()).type(h.dtype)\n",
        "        h = self.nonlinearity(h)\n",
        "\n",
        "        # if self.upsample is not None:\n",
        "        #     x = self.upsample(x)\n",
        "        #     h = self.upsample(h)\n",
        "        # elif self.downsample is not None:\n",
        "        #     x = self.downsample(x)\n",
        "        #     h = self.downsample(h)\n",
        "\n",
        "        h = self.conv1(h)\n",
        "\n",
        "        # print(\"LOG temb:\",temb is None)\n",
        "\n",
        "        # if temb is not None:\n",
        "        #     temb = self.time_emb_proj(self.nonlinearity(temb))[:, :, None, None]\n",
        "        #     h = h + temb\n",
        "\n",
        "        # make sure hidden states is in float32\n",
        "        # when running in half-precision\n",
        "        h = self.norm2(h.float()).type(h.dtype)\n",
        "        h = self.nonlinearity(h)\n",
        "\n",
        "        h = self.dropout(h)\n",
        "        h = self.conv2(h)\n",
        "\n",
        "        if self.conv_shortcut is not None:\n",
        "            x = self.conv_shortcut(x)\n",
        "\n",
        "        out = (x + h) / self.output_scale_factor\n",
        "\n",
        "        return out\n",
        "\n",
        "    def set_weight(self, resnet):\n",
        "        self.norm1.weight.data = resnet.norm1.weight.data\n",
        "        self.norm1.bias.data = resnet.norm1.bias.data\n",
        "\n",
        "        self.conv1.weight.data = resnet.conv1.weight.data\n",
        "        self.conv1.bias.data = resnet.conv1.bias.data\n",
        "\n",
        "        if self.time_emb_proj is not None:\n",
        "            self.time_emb_proj.weight.data = resnet.temb_proj.weight.data\n",
        "            self.time_emb_proj.bias.data = resnet.temb_proj.bias.data\n",
        "\n",
        "        self.norm2.weight.data = resnet.norm2.weight.data\n",
        "        self.norm2.bias.data = resnet.norm2.bias.data\n",
        "\n",
        "        self.conv2.weight.data = resnet.conv2.weight.data\n",
        "        self.conv2.bias.data = resnet.conv2.bias.data\n",
        "\n",
        "        if self.use_nin_shortcut:\n",
        "            self.conv_shortcut.weight.data = resnet.nin_shortcut.weight.data\n",
        "            self.conv_shortcut.bias.data = resnet.nin_shortcut.bias.data\n",
        "            \n",
        "            \n",
        "class Downsample2D(nn.Module):\n",
        "    \"\"\"\n",
        "    A downsampling layer with an optional convolution.\n",
        "    :param channels: channels in the inputs and outputs. :param use_conv: a bool determining if a convolution is\n",
        "    applied. :param dims: determines if the signal is 1D, 2D, or 3D. If 3D, then\n",
        "                 downsampling occurs in the inner-two dimensions.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, channels, use_conv=False, out_channels=None, padding=1, name=\"conv\"):\n",
        "        super().__init__()\n",
        "        self.channels = channels\n",
        "        self.out_channels = out_channels or channels\n",
        "        self.use_conv = use_conv\n",
        "        self.padding = padding\n",
        "        stride = 2\n",
        "        self.name = name\n",
        "\n",
        "        if use_conv:\n",
        "            conv = nn.Conv2d(self.channels, self.out_channels, 3, stride=stride, padding=padding)\n",
        "        else:\n",
        "            assert self.channels == self.out_channels\n",
        "            conv = nn.AvgPool2d(kernel_size=stride, stride=stride)\n",
        "\n",
        "        # TODO(Suraj, Patrick) - clean up after weight dicts are correctly renamed\n",
        "        if name == \"conv\":\n",
        "            self.Conv2d_0 = conv\n",
        "            self.conv = conv\n",
        "        elif name == \"Conv2d_0\":\n",
        "            self.conv = conv\n",
        "        else:\n",
        "            self.conv = conv\n",
        "\n",
        "    def forward(self, x):\n",
        "        assert x.shape[1] == self.channels\n",
        "        if self.use_conv and self.padding == 0:\n",
        "            pad = (0, 1, 0, 1)\n",
        "            x = F.pad(x, pad, mode=\"constant\", value=0)\n",
        "\n",
        "        assert x.shape[1] == self.channels\n",
        "        x = self.conv(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "class Upsample2D(nn.Module):\n",
        "\n",
        "    \"\"\"\n",
        "    An upsampling layer with an optional convolution.\n",
        "    :param channels: channels in the inputs and outputs. :param use_conv: a bool determining if a convolution is\n",
        "    applied. :param dims: determines if the signal is 1D, 2D, or 3D. If 3D, then\n",
        "                 upsampling occurs in the inner-two dimensions.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, channels, use_conv=False, use_conv_transpose=False, out_channels=None, name=\"conv\"):\n",
        "        super().__init__()\n",
        "        self.channels = channels\n",
        "        self.out_channels = out_channels or channels\n",
        "        self.use_conv = use_conv\n",
        "        self.use_conv_transpose = use_conv_transpose\n",
        "        self.name = name\n",
        "\n",
        "        conv = None\n",
        "        if use_conv_transpose:\n",
        "            conv = nn.ConvTranspose2d(channels, self.out_channels, 4, 2, 1)\n",
        "        elif use_conv:\n",
        "            conv = nn.Conv2d(self.channels, self.out_channels, 3, padding=1)\n",
        "\n",
        "        # TODO(Suraj, Patrick) - clean up after weight dicts are correctly renamed\n",
        "        if name == \"conv\":\n",
        "            self.conv = conv\n",
        "        else:\n",
        "            self.Conv2d_0 = conv\n",
        "\n",
        "    def forward(self, x):\n",
        "        assert x.shape[1] == self.channels\n",
        "        if self.use_conv_transpose:\n",
        "            return self.conv(x)\n",
        "\n",
        "        x = F.interpolate(x, scale_factor=2.0, mode=\"nearest\")\n",
        "\n",
        "        # TODO(Suraj, Patrick) - clean up after weight dicts are correctly renamed\n",
        "        if self.use_conv:\n",
        "            if self.name == \"conv\":\n",
        "                x = self.conv(x)\n",
        "            else:\n",
        "                x = self.Conv2d_0(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class AttentionBlockNew(nn.Module):\n",
        "    \"\"\"\n",
        "    An attention block that allows spatial positions to attend to each other. Originally ported from here, but adapted\n",
        "    to the N-d case.\n",
        "    https://github.com/hojonathanho/diffusion/blob/1e0dceb3b3495bbe19116a5e1b3596cd0706c543/diffusion_tf/models/unet.py#L66.\n",
        "    Uses three q, k, v linear layers to compute attention\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        channels,\n",
        "        num_head_channels=None,\n",
        "        num_groups=32,\n",
        "        rescale_output_factor=1.0,\n",
        "        eps=1e-5,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.channels = channels\n",
        "\n",
        "        self.num_heads = channels // num_head_channels if num_head_channels is not None else 1\n",
        "        self.num_head_size = num_head_channels\n",
        "        self.group_norm = nn.GroupNorm(num_channels=channels, num_groups=num_groups, eps=eps, affine=True)\n",
        "\n",
        "        # define q,k,v as linear layers\n",
        "        self.query = nn.Linear(channels, channels)\n",
        "        self.key = nn.Linear(channels, channels)\n",
        "        self.value = nn.Linear(channels, channels)\n",
        "\n",
        "        self.rescale_output_factor = rescale_output_factor\n",
        "        self.proj_attn = nn.Linear(channels, channels, 1)\n",
        "\n",
        "    def transpose_for_scores(self, projection: torch.Tensor) -> torch.Tensor:\n",
        "        new_projection_shape = projection.size()[:-1] + (self.num_heads, -1)\n",
        "        # move heads to 2nd position (B, T, H * D) -> (B, T, H, D) -> (B, H, T, D)\n",
        "        new_projection = projection.view(new_projection_shape).permute(0, 2, 1, 3)\n",
        "        return new_projection\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        residual = hidden_states\n",
        "        batch, channel, height, width = hidden_states.shape\n",
        "\n",
        "        # norm\n",
        "        hidden_states = self.group_norm(hidden_states)\n",
        "\n",
        "        hidden_states = hidden_states.view(batch, channel, height * width).transpose(1, 2)\n",
        "\n",
        "        # proj to q, k, v\n",
        "        query_proj = self.query(hidden_states)\n",
        "        key_proj = self.key(hidden_states)\n",
        "        value_proj = self.value(hidden_states)\n",
        "\n",
        "        # transpose\n",
        "        query_states = self.transpose_for_scores(query_proj)\n",
        "        key_states = self.transpose_for_scores(key_proj)\n",
        "        value_states = self.transpose_for_scores(value_proj)\n",
        "\n",
        "        # get scores\n",
        "        scale = 1 / math.sqrt(math.sqrt(self.channels / self.num_heads))\n",
        "        attention_scores = torch.matmul(query_states * scale, key_states.transpose(-1, -2) * scale)\n",
        "        attention_probs = torch.softmax(attention_scores.float(), dim=-1).type(attention_scores.dtype)\n",
        "\n",
        "        # compute attention output\n",
        "        context_states = torch.matmul(attention_probs, value_states)\n",
        "\n",
        "        context_states = context_states.permute(0, 2, 1, 3).contiguous()\n",
        "        new_context_states_shape = context_states.size()[:-2] + (self.channels,)\n",
        "        context_states = context_states.view(new_context_states_shape)\n",
        "\n",
        "        # compute next hidden_states\n",
        "        hidden_states = self.proj_attn(context_states)\n",
        "        hidden_states = hidden_states.transpose(-1, -2).reshape(batch, channel, height, width)\n",
        "\n",
        "        # res connect and rescale\n",
        "        hidden_states = (hidden_states + residual) / self.rescale_output_factor\n",
        "        return hidden_states\n",
        "\n",
        "    def set_weight(self, attn_layer):\n",
        "        self.group_norm.weight.data = attn_layer.norm.weight.data\n",
        "        self.group_norm.bias.data = attn_layer.norm.bias.data\n",
        "\n",
        "        if hasattr(attn_layer, \"q\"):\n",
        "            self.query.weight.data = attn_layer.q.weight.data[:, :, 0, 0]\n",
        "            self.key.weight.data = attn_layer.k.weight.data[:, :, 0, 0]\n",
        "            self.value.weight.data = attn_layer.v.weight.data[:, :, 0, 0]\n",
        "\n",
        "            self.query.bias.data = attn_layer.q.bias.data\n",
        "            self.key.bias.data = attn_layer.k.bias.data\n",
        "            self.value.bias.data = attn_layer.v.bias.data\n",
        "\n",
        "            self.proj_attn.weight.data = attn_layer.proj_out.weight.data[:, :, 0, 0]\n",
        "            self.proj_attn.bias.data = attn_layer.proj_out.bias.data\n",
        "        elif hasattr(attn_layer, \"NIN_0\"):\n",
        "            self.query.weight.data = attn_layer.NIN_0.W.data.T\n",
        "            self.key.weight.data = attn_layer.NIN_1.W.data.T\n",
        "            self.value.weight.data = attn_layer.NIN_2.W.data.T\n",
        "\n",
        "            self.query.bias.data = attn_layer.NIN_0.b.data\n",
        "            self.key.bias.data = attn_layer.NIN_1.b.data\n",
        "            self.value.bias.data = attn_layer.NIN_2.b.data\n",
        "\n",
        "            self.proj_attn.weight.data = attn_layer.NIN_3.W.data.T\n",
        "            self.proj_attn.bias.data = attn_layer.NIN_3.b.data\n",
        "\n",
        "            self.group_norm.weight.data = attn_layer.GroupNorm_0.weight.data\n",
        "            self.group_norm.bias.data = attn_layer.GroupNorm_0.bias.data\n",
        "        else:\n",
        "            qkv_weight = attn_layer.qkv.weight.data.reshape(\n",
        "                self.num_heads, 3 * self.channels // self.num_heads, self.channels\n",
        "            )\n",
        "            qkv_bias = attn_layer.qkv.bias.data.reshape(self.num_heads, 3 * self.channels // self.num_heads)\n",
        "\n",
        "            q_w, k_w, v_w = qkv_weight.split(self.channels // self.num_heads, dim=1)\n",
        "            q_b, k_b, v_b = qkv_bias.split(self.channels // self.num_heads, dim=1)\n",
        "\n",
        "            self.query.weight.data = q_w.reshape(-1, self.channels)\n",
        "            self.key.weight.data = k_w.reshape(-1, self.channels)\n",
        "            self.value.weight.data = v_w.reshape(-1, self.channels)\n",
        "\n",
        "            self.query.bias.data = q_b.reshape(-1)\n",
        "            self.key.bias.data = k_b.reshape(-1)\n",
        "            self.value.bias.data = v_b.reshape(-1)\n",
        "\n",
        "            self.proj_attn.weight.data = attn_layer.proj.weight.data[:, :, 0]\n",
        "            self.proj_attn.bias.data = attn_layer.proj.bias.data\n",
        "\n",
        "class UNetMidBlock2D(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels: int,\n",
        "        temb_channels: int,\n",
        "        dropout: float = 0.0,\n",
        "        num_layers: int = 1,\n",
        "        resnet_eps: float = 1e-6,\n",
        "        resnet_time_scale_shift: str = \"default\",\n",
        "        resnet_act_fn: str = \"swish\",\n",
        "        resnet_groups: int = 32,\n",
        "        resnet_pre_norm: bool = True,\n",
        "        attn_num_head_channels=1,\n",
        "        attention_type=\"default\",\n",
        "        output_scale_factor=1.0,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.attention_type = attention_type\n",
        "        resnet_groups = resnet_groups if resnet_groups is not None else min(in_channels // 4, 32)\n",
        "\n",
        "        # there is always at least one resnet\n",
        "        resnets = [\n",
        "            ResnetBlock(\n",
        "                in_channels=in_channels,\n",
        "                out_channels=in_channels,\n",
        "                temb_channels=temb_channels,\n",
        "                eps=resnet_eps,\n",
        "                groups=resnet_groups,\n",
        "                dropout=dropout,\n",
        "                time_embedding_norm=resnet_time_scale_shift,\n",
        "                non_linearity=resnet_act_fn,\n",
        "                output_scale_factor=output_scale_factor,\n",
        "                pre_norm=resnet_pre_norm,\n",
        "            )\n",
        "        ]\n",
        "        attentions = []\n",
        "\n",
        "        for _ in range(num_layers):\n",
        "            attentions.append(\n",
        "                AttentionBlockNew(\n",
        "                    in_channels,\n",
        "                    num_head_channels=attn_num_head_channels,\n",
        "                    rescale_output_factor=output_scale_factor,\n",
        "                    eps=resnet_eps,\n",
        "                    num_groups=resnet_groups,\n",
        "                )\n",
        "            )\n",
        "            resnets.append(\n",
        "                ResnetBlock(\n",
        "                    in_channels=in_channels,\n",
        "                    out_channels=in_channels,\n",
        "                    temb_channels=temb_channels,\n",
        "                    eps=resnet_eps,\n",
        "                    groups=resnet_groups,\n",
        "                    dropout=dropout,\n",
        "                    time_embedding_norm=resnet_time_scale_shift,\n",
        "                    non_linearity=resnet_act_fn,\n",
        "                    output_scale_factor=output_scale_factor,\n",
        "                    pre_norm=resnet_pre_norm,\n",
        "                )\n",
        "            )\n",
        "\n",
        "        self.attentions = nn.ModuleList(attentions)\n",
        "        self.resnets = nn.ModuleList(resnets)\n",
        "\n",
        "    def forward(self, hidden_states, temb=None, encoder_states=None):\n",
        "        hidden_states = self.resnets[0](hidden_states, temb)\n",
        "        for attn, resnet in zip(self.attentions, self.resnets[1:]):\n",
        "            if self.attention_type == \"default\":\n",
        "                hidden_states = attn(hidden_states)\n",
        "            else:\n",
        "                hidden_states = attn(hidden_states, encoder_states)\n",
        "            hidden_states = resnet(hidden_states, temb)\n",
        "\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels=3,\n",
        "        out_channels=4,\n",
        "        down_block_types=(\"DownEncoderBlock2D\",),\n",
        "        block_out_channels=(64,),\n",
        "        layers_per_block=1,\n",
        "        act_fn=\"silu\",\n",
        "        double_z=True,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.layers_per_block = layers_per_block\n",
        "        self.conv_in = torch.nn.Conv2d(in_channels, block_out_channels[0], kernel_size=3, stride=1, padding=1)\n",
        "        self.mid_block = None\n",
        "        self.down_blocks = nn.ModuleList([])\n",
        "\n",
        "        # down\n",
        "        output_channel = block_out_channels[0]\n",
        "        for i, _ in enumerate(down_block_types):\n",
        "            input_channel = output_channel\n",
        "            output_channel = block_out_channels[i]\n",
        "            is_final_block = i == len(block_out_channels) - 1\n",
        "\n",
        "            down_block = DownEncoderBlock2D(\n",
        "                num_layers=self.layers_per_block,\n",
        "                in_channels=input_channel,\n",
        "                out_channels=output_channel,\n",
        "                add_downsample=not is_final_block,\n",
        "                resnet_eps=1e-6,\n",
        "                resnet_act_fn=act_fn,\n",
        "                downsample_padding=0,)\n",
        "\n",
        "            self.down_blocks.append(down_block)\n",
        "\n",
        "        # mid\n",
        "        self.mid_block = UNetMidBlock2D(\n",
        "            in_channels=block_out_channels[-1],\n",
        "            resnet_eps=1e-6,\n",
        "            resnet_act_fn=act_fn,\n",
        "            output_scale_factor=1,\n",
        "            resnet_time_scale_shift=\"default\",\n",
        "            attn_num_head_channels=None,\n",
        "            resnet_groups=32,\n",
        "            temb_channels=None,\n",
        "        )\n",
        "\n",
        "        # out\n",
        "        num_groups_out = 32\n",
        "        self.conv_norm_out = nn.GroupNorm(num_channels=block_out_channels[-1], num_groups=num_groups_out, eps=1e-6)\n",
        "        self.conv_act = nn.SiLU()\n",
        "\n",
        "        conv_out_channels = 2 * out_channels if double_z else out_channels\n",
        "        self.conv_out = nn.Conv2d(block_out_channels[-1], conv_out_channels, 3, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \n",
        "        sample = x\n",
        "        sample = self.conv_in(sample)\n",
        "        \n",
        "        # down\n",
        "        for down_block in self.down_blocks:\n",
        "            sample = down_block(sample)\n",
        "\n",
        "        # middle\n",
        "        sample = self.mid_block(sample)\n",
        "\n",
        "        # post-process\n",
        "        sample = self.conv_norm_out(sample)\n",
        "        sample = self.conv_act(sample)\n",
        "        sample = self.conv_out(sample)\n",
        "\n",
        "        return sample\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels=3,\n",
        "        out_channels=3,\n",
        "        up_block_types=(\"UpDecoderBlock2D\",),\n",
        "        block_out_channels=(64,),\n",
        "        layers_per_block=2,\n",
        "        act_fn=\"silu\",\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.layers_per_block = layers_per_block\n",
        "\n",
        "        self.conv_in = nn.Conv2d(in_channels, block_out_channels[-1], kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "        self.mid_block = None\n",
        "        self.up_blocks = nn.ModuleList([])\n",
        "\n",
        "        # mid\n",
        "        self.mid_block = UNetMidBlock2D(\n",
        "            in_channels=block_out_channels[-1],\n",
        "            resnet_eps=1e-6,\n",
        "            resnet_act_fn=act_fn,\n",
        "            output_scale_factor=1,\n",
        "            resnet_time_scale_shift=\"default\",\n",
        "            attn_num_head_channels=None,\n",
        "            resnet_groups=32,\n",
        "            temb_channels=None,\n",
        "        )\n",
        "\n",
        "        # up\n",
        "        reversed_block_out_channels = list(reversed(block_out_channels))\n",
        "        output_channel = reversed_block_out_channels[0]\n",
        "        for i, up_block_type in enumerate(up_block_types):\n",
        "            prev_output_channel = output_channel\n",
        "            output_channel = reversed_block_out_channels[i]\n",
        "\n",
        "            is_final_block = i == len(block_out_channels) - 1\n",
        "\n",
        "            up_block = UpDecoderBlock2D(\n",
        "            num_layers=self.layers_per_block + 1,\n",
        "            in_channels=prev_output_channel,\n",
        "            out_channels=output_channel,\n",
        "            add_upsample=not is_final_block,\n",
        "            resnet_eps=1e-6,\n",
        "            resnet_act_fn=act_fn,\n",
        "        )\n",
        "\n",
        "            self.up_blocks.append(up_block)\n",
        "            prev_output_channel = output_channel\n",
        "\n",
        "        # out\n",
        "        num_groups_out = 32\n",
        "        self.conv_norm_out = nn.GroupNorm(num_channels=block_out_channels[0], num_groups=num_groups_out, eps=1e-6)\n",
        "        self.conv_act = nn.SiLU()\n",
        "        self.conv_out = nn.Conv2d(block_out_channels[0], out_channels, 3, padding=1)\n",
        "\n",
        "    def forward(self, z):\n",
        "        sample = z\n",
        "        sample = self.conv_in(sample)\n",
        "\n",
        "        # middle\n",
        "        sample = self.mid_block(sample)\n",
        "\n",
        "        # up\n",
        "        for up_block in self.up_blocks:\n",
        "            sample = up_block(sample)\n",
        "\n",
        "        # post-process\n",
        "        sample = self.conv_norm_out(sample)\n",
        "        sample = self.conv_act(sample)\n",
        "        sample = self.conv_out(sample)\n",
        "\n",
        "        return sample"
      ],
      "metadata": {
        "id": "5_K2AE--WiX4"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DiagonalGaussianDistribution(object):\n",
        "    def __init__(self, parameters, deterministic=False):\n",
        "        self.parameters = parameters\n",
        "        self.mean, self.logvar = torch.chunk(parameters, 2, dim=1)\n",
        "        self.logvar = torch.clamp(self.logvar, -30.0, 20.0)\n",
        "        self.deterministic = deterministic\n",
        "        self.std = torch.exp(0.5 * self.logvar)\n",
        "        self.var = torch.exp(self.logvar)\n",
        "        if self.deterministic:\n",
        "            self.var = self.std = torch.zeros_like(self.mean).to(device=self.parameters.device)\n",
        "\n",
        "    def sample(self):\n",
        "        x = self.mean + self.std * torch.randn(self.mean.shape).to(device=self.parameters.device)\n",
        "        return x\n",
        "\n",
        "    def kl(self, other=None):\n",
        "        if self.deterministic:\n",
        "            return torch.Tensor([0.0])\n",
        "        else:\n",
        "            if other is None:\n",
        "                return 0.5 * torch.sum(torch.pow(self.mean, 2) + self.var - 1.0 - self.logvar, dim=[1, 2, 3])\n",
        "            else:\n",
        "                return 0.5 * torch.sum(\n",
        "                    torch.pow(self.mean - other.mean, 2) / other.var\n",
        "                    + self.var / other.var\n",
        "                    - 1.0\n",
        "                    - self.logvar\n",
        "                    + other.logvar,\n",
        "                    dim=[1, 2, 3],\n",
        "                )\n",
        "\n",
        "    def nll(self, sample, dims=[1, 2, 3]):\n",
        "        if self.deterministic:\n",
        "            return torch.Tensor([0.0])\n",
        "        logtwopi = np.log(2.0 * np.pi)\n",
        "        return 0.5 * torch.sum(logtwopi + self.logvar + torch.pow(sample - self.mean, 2) / self.var, dim=dims)\n",
        "\n",
        "    def mode(self):\n",
        "        return self.mean\n",
        "\n",
        "\n",
        "\n",
        "encoder = Encoder(in_channels=3,\n",
        "        out_channels=4,\n",
        "        down_block_types=(\"DownEncoderBlock2D\",),\n",
        "        block_out_channels=(64,),\n",
        "        layers_per_block=1,\n",
        "        act_fn=\"silu\",\n",
        "        double_z=True)\n",
        "\n",
        "\n",
        "decoder = Decoder(in_channels=4,\n",
        "            out_channels=3,\n",
        "            up_block_types=(\"UpDecoderBlock2D\",),\n",
        "            block_out_channels=(64,),\n",
        "            layers_per_block=1,\n",
        "            act_fn=\"silu\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    ##########################\n",
        "    ##########################\n",
        "    ##########################\n",
        "#### AutoEncoder wrap- up class ####\n",
        "    ##########################\n",
        "    ##########################\n",
        "    ##########################\n",
        "\n",
        "class AutoencoderKL(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels=3,\n",
        "        out_channels=3,\n",
        "        down_block_types=(\"DownEncoderBlock2D\",\"DownEncoderBlock2D\",\"DownEncoderBlock2D\",\"DownEncoderBlock2D\",),\n",
        "        up_block_types=(\"UpDecoderBlock2D\",\"UpDecoderBlock2D\",\"UpDecoderBlock2D\",\"UpDecoderBlock2D\",),\n",
        "        block_out_channels=(128, 256, 512, 512,),\n",
        "        layers_per_block=2,\n",
        "        act_fn=\"silu\",\n",
        "        latent_channels=4,\n",
        "        sample_size=512,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        # pass init params to Encoder\n",
        "        self.encoder = Encoder(\n",
        "            in_channels=in_channels,\n",
        "            out_channels=latent_channels,\n",
        "            down_block_types=down_block_types,\n",
        "            block_out_channels=block_out_channels,\n",
        "            layers_per_block=layers_per_block,\n",
        "            act_fn=act_fn,\n",
        "            double_z=True,\n",
        "        )\n",
        "\n",
        "        # pass init params to Decoder\n",
        "        self.decoder = Decoder(\n",
        "            in_channels=latent_channels,\n",
        "            out_channels=out_channels,\n",
        "            up_block_types=up_block_types,\n",
        "            block_out_channels=block_out_channels,\n",
        "            layers_per_block=layers_per_block,\n",
        "            act_fn=act_fn,\n",
        "        )\n",
        "\n",
        "        self.quant_conv = torch.nn.Conv2d(2 * latent_channels, 2 * latent_channels, 1)\n",
        "        self.post_quant_conv = torch.nn.Conv2d(latent_channels, latent_channels, 1) \n",
        "        self.kl = 0\n",
        "\n",
        "    def encode(self, x):\n",
        "        h = self.encoder(x)\n",
        "        moments = self.quant_conv(h)\n",
        "        posterior = DiagonalGaussianDistribution(moments)\n",
        "        return posterior\n",
        "\n",
        "    def decode(self, z):\n",
        "        z = self.post_quant_conv(z)\n",
        "        dec = self.decoder(z)\n",
        "        return dec\n",
        "\n",
        "    def forward(self, sample, sample_posterior=False):\n",
        "        x = sample\n",
        "        posterior = self.encode(x)\n",
        "        if sample_posterior:\n",
        "            z = posterior.sample()\n",
        "        else:\n",
        "            z = posterior.mode()\n",
        "        dec = self.decode(z)\n",
        "        self.kl=posterior.kl()\n",
        "        # print(\"LOG KL:\",self.kl.sum())\n",
        "        return dec\n"
      ],
      "metadata": {
        "id": "wpwvB6BhWvDF"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Training function**"
      ],
      "metadata": {
        "id": "crEhgj4FVZTB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##### train\n",
        "from tqdm.auto import tqdm\n",
        "PATH = './vae_net.pth' # default path\n",
        "losses=[]\n",
        "\n",
        "def train(autoencoder, data, epochs=20,exp=None,lr=0.001,schedualer_step=1000):\n",
        "    \n",
        "    def adjust(sample):\n",
        "        # sample=np.expand_dims(np.asarray(sample), axis=0)\n",
        "        sample=sample.numpy()/ 1.0\n",
        "        # sample=sample.astype('float32')\n",
        "        sample = torch.from_numpy(sample).float() # Batch - RGB channel - WxH \n",
        "        sample = 2 * (sample - 0.5) # values between (-1, 1)\n",
        "        return sample\n",
        "      \n",
        "    def variational_loss(x,x_hat):\n",
        "        return F.mse_loss(x,x_hat) + torch.mean(autoencoder.kl,dim=0)\n",
        "\n",
        "    if exp:\n",
        "        model_file='./{}_net.pth'.format(exp)\n",
        "    else:\n",
        "        model_file=PATH\n",
        "    \n",
        "    global opt\n",
        "    opt = torch.optim.Adam(autoencoder.parameters(),lr=lr)\n",
        "    scheduler = torch.optim.lr_scheduler.ExponentialLR(opt, gamma=0.9,verbose=True)\n",
        "    epoch = 0\n",
        "\n",
        "    if os.path.exists(model_file):\n",
        "      checkpoint = torch.load(model_file)\n",
        "      autoencoder.load_state_dict(checkpoint['model_state_dict'])\n",
        "      opt.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "      scheduler.load_state_dict(checkpoint['scheduler'])\n",
        "      epoch = checkpoint['epoch']\n",
        "      loss = checkpoint['loss']\n",
        "\n",
        "    for epoch in range(epoch, epochs):\n",
        "        # print(\"Log epoch:\",epoch)\n",
        "\n",
        "       with tqdm(data, unit=\"batch\",total=len(data)) as tepoch:\n",
        "        for i, x in enumerate(tepoch):\n",
        "            tepoch.set_description(f\"Epoch {epoch}\")\n",
        "            x=adjust(x[0]) # Values between -1..1 \n",
        "            x = torch.Tensor(x).to(device) # GPU\n",
        "            opt.zero_grad()\n",
        "            x_hat = autoencoder(x,sample_posterior=True)\n",
        "            reconstruction_loss=((x - x_hat)**2).sum()\n",
        "            kl_loss=autoencoder.kl.sum()\n",
        "            # print(\"LOG reconstruction loss:\", reconstruction_loss)\n",
        "            # print(\"LOG KL:\",kl_loss)\n",
        "            # loss = reconstruction_loss + kl_loss\n",
        "            loss = variational_loss(x,x_hat)\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            loss=loss.item()\n",
        "            losses.append(loss)\n",
        "            tepoch.set_postfix(loss=loss)\n",
        "\n",
        "            # print(loss)\n",
        "\n",
        "            if not i % schedualer_step and i != 0: \n",
        "                torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': autoencoder.state_dict(),\n",
        "                'optimizer_state_dict': opt.state_dict(),\n",
        "                'loss': loss,\n",
        "                'scheduler':scheduler.state_dict()}, model_file)\n",
        "                \n",
        "                print(\"model saved!\")\n",
        "                scheduler.step()\n",
        "        \n",
        "        torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': autoencoder.state_dict(),\n",
        "                'optimizer_state_dict': opt.state_dict(),\n",
        "                'loss': loss,\n",
        "                'scheduler':scheduler.state_dict()}, model_file)\n",
        "        print(\"model saved!\")\n",
        "   \n",
        "    return autoencoder"
      ],
      "metadata": {
        "id": "hieXAg7cVX-x"
      },
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**DataLoader Cifar10** "
      ],
      "metadata": {
        "id": "AKgZYAGTVpnx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ToDo: https://gist.github.com/kevinzakka/d33bf8d6c7f06a9d8c76d97a7879f5cb\n",
        "# normalize dataset.\n",
        "\n",
        "def load_data(BS=32,path = \"./cifar-10-batches-py\",num_workers=4):\n",
        "    train = torch.utils.data.DataLoader(\n",
        "        torchvision.datasets.CIFAR10(path,\n",
        "               transform=torchvision.transforms.ToTensor(),\n",
        "               download=True),\n",
        "        batch_size=BS,\n",
        "        shuffle=True,\n",
        "        num_workers=num_workers)\n",
        "    \n",
        "    test = torch.utils.data.DataLoader(\n",
        "        torchvision.datasets.CIFAR10(path,\n",
        "               transform=torchvision.transforms.ToTensor(),\n",
        "               download=True,train=False),\n",
        "        batch_size=BS,\n",
        "        shuffle=True,\n",
        "        num_workers=num_workers)\n",
        "        \n",
        "    print(train.dataset)\n",
        "    print(test.dataset)\n",
        "    print(\"train data size:\",next(iter(train))[0].size())\n",
        "\n",
        "    return train,test\n"
      ],
      "metadata": {
        "id": "0ZSbEP8ik-CG"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train/Test Split**\n"
      ],
      "metadata": {
        "id": "dVGV1ZUMj7Fg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training Variational AutoEncoder on Cifar10 data."
      ],
      "metadata": {
        "id": "KWrWRwtFA6hh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data,test_data=load_data(BS=32,num_workers=4)\n",
        "vae=AutoencoderKL().to(device)\n",
        "vae = train(vae, train_data,epochs=10,exp=\"test8\",lr=1e-3,schedualer_step=300)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "1dd545a4065a453bb0522b6dad369d18",
            "cbb3e814172749d89ba02aa02fb1d2fa",
            "3f00a014436e4535b8921b1620878416",
            "47a2b54350664393889abc818011cfd8",
            "b06b4a5cea214634ad745a3d78cac170",
            "2a5402e930e2420486c4ed62b61e8d3c",
            "2f3367f5cc5e4cd6931c377d62673965",
            "daaaefe5ee714b40aa3d277b9223c365",
            "490590f21ac341298e02e63a6a12c822",
            "341a860811a543af9a8e3d95b7b97c0d",
            "a570655de1f44464a31de57d7e4e8a3e"
          ]
        },
        "id": "sPWD7vMck51E",
        "outputId": "d93a18e1-9401-45eb-d60d-17e1b747e799"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:566: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Dataset CIFAR10\n",
            "    Number of datapoints: 50000\n",
            "    Root location: ./cifar-10-batches-py\n",
            "    Split: Train\n",
            "    StandardTransform\n",
            "Transform: ToTensor()\n",
            "Dataset CIFAR10\n",
            "    Number of datapoints: 10000\n",
            "    Root location: ./cifar-10-batches-py\n",
            "    Split: Test\n",
            "    StandardTransform\n",
            "Transform: ToTensor()\n",
            "train data size: torch.Size([32, 3, 32, 32])\n",
            "Adjusting learning rate of group 0 to 1.0000e-03.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/1563 [00:00<?, ?batch/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1dd545a4065a453bb0522b6dad369d18"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model saved!\n",
            "Adjusting learning rate of group 0 to 9.0000e-04.\n",
            "model saved!\n",
            "Adjusting learning rate of group 0 to 8.1000e-04.\n",
            "model saved!\n",
            "Adjusting learning rate of group 0 to 7.2900e-04.\n",
            "model saved!\n",
            "Adjusting learning rate of group 0 to 6.5610e-04.\n",
            "model saved!\n",
            "Adjusting learning rate of group 0 to 5.9049e-04.\n",
            "model saved!\n",
            "Adjusting learning rate of group 0 to 5.3144e-04.\n",
            "model saved!\n",
            "Adjusting learning rate of group 0 to 4.7830e-04.\n",
            "model saved!\n",
            "Adjusting learning rate of group 0 to 4.3047e-04.\n",
            "model saved!\n",
            "Adjusting learning rate of group 0 to 3.8742e-04.\n",
            "model saved!\n",
            "Adjusting learning rate of group 0 to 3.4868e-04.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-d4b7ced65a22>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBS\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mvae\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mAutoencoderKL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mvae\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvae\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"test7\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mschedualer_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-41-f74cc9785d50>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(autoencoder, data, epochs, exp, lr, schedualer_step)\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;31m# loss = reconstruction_loss + kl_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvariational_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_hat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m             \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    394\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 396\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    173\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m def grad(\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Check the results"
      ],
      "metadata": {
        "id": "CMxI7UsrTkVe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib.pyplot import plot\n",
        "\n",
        "plot(losses)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "E2aDBAF-ATeR",
        "outputId": "49eeef72-ab52-4491-9965-70d43fb08e90"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fa9341ed510>]"
            ]
          },
          "metadata": {},
          "execution_count": 43
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib.pyplot import imshow\n",
        "%matplotlib inline\n",
        "def decode_img(img):\n",
        "    from PIL import Image\n",
        "    img = (img / 2 + 0.5).clamp(0, 1)\n",
        "    img = img.detach().cpu().permute(1, 2, 0).numpy()\n",
        "    img = (img * 255).round().astype('uint8')\n",
        "    pil_image = Image.fromarray(img)\n",
        "    return pil_image\n",
        "\n",
        "def adjust(sample):\n",
        "        # sample=np.expand_dims(np.asarray(sample), axis=0)\n",
        "        sample=sample.numpy()/ 1.0\n",
        "        # sample=sample.astype('float32')\n",
        "        sample = torch.from_numpy(sample).float() # Batch - RGB channel - WxH \n",
        "        sample = 2 * (sample - 0.5) # values between (-1, 1)\n",
        "        return sample\n"
      ],
      "metadata": {
        "id": "8XpNYhETpyvG"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -la"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nu88VDINCZaJ",
        "outputId": "5fcd88a4-4bde-4aec-a30e-ef211b6b4f50"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 3300700\n",
            "drwxr-xr-x 1 root root       4096 Nov 18 22:35 .\n",
            "drwxr-xr-x 1 root root       4096 Nov 18 16:59 ..\n",
            "drwxr-xr-x 3 root root       4096 Nov 18 17:25 cifar-10-batches-py\n",
            "drwxr-xr-x 4 root root       4096 Nov 16 14:34 .config\n",
            "drwx------ 5 root root       4096 Nov 18 17:05 gdrive\n",
            "drwxr-xr-x 1 root root       4096 Nov 16 14:35 sample_data\n",
            "-rw-r--r-- 1 root root  367413440 Nov 18 17:30 test4_net.pth\n",
            "-rw-r--r-- 1 root root 1004151233 Nov 18 20:24 test5_net.pth\n",
            "-rw-r--r-- 1 root root 1004151233 Nov 18 22:13 test6_net.pth\n",
            "-rw-r--r-- 1 root root 1004151233 Nov 18 22:41 test7_net.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls gdrive/MyDrive/*.pth"
      ],
      "metadata": {
        "id": "rnSRCnRx7egW",
        "outputId": "4da98d1c-7578-4d0a-85f3-8b7ef465719c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gdrive/MyDrive/None_net.pth  gdrive/MyDrive/test1_net.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "exp=\"test5\"\n",
        "checkpoint = torch.load(f\"./{exp}_net.pth\")\n",
        "vae.load_state_dict(checkpoint['model_state_dict'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zdp_u-z8mbzC",
        "outputId": "f8d2d6f0-faec-4e1d-c9a9-0bae94abd187"
      },
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 140
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_imgs = next(iter(test_data))[0]\n",
        "img=batch_imgs[0]\n",
        "decode_img(img)"
      ],
      "metadata": {
        "id": "WNF-e-xiC-2P",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49
        },
        "outputId": "fe324fa7-a4de-42b4-f578-a0da2bd2a404"
      },
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=32x32 at 0x7FAADE53A110>"
            ],
            "image/png": "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\n"
          },
          "metadata": {},
          "execution_count": 143
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output=vae(adjust(torch.Tensor(img)[None, :]).to(device))[0]\n",
        "decode_img(output)"
      ],
      "metadata": {
        "id": "gk7rLY7FqzbZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49
        },
        "outputId": "30cf8d6c-964d-4730-edc1-bf94d11dcfb8"
      },
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=32x32 at 0x7FA80850F050>"
            ],
            "image/png": "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\n"
          },
          "metadata": {},
          "execution_count": 144
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jeb2mkyhm-1B",
        "outputId": "3f6ef0ae-2f0e-41ae-fe26-7c52e7804769"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cifar-10-batches-py  gdrive  sample_data  test2_net.pth  test3_net.pth\n"
          ]
        }
      ]
    }
  ]
}