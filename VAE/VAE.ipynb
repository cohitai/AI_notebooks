{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPl3CfwYqV/CEQ4iCgDr3RQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "016e155f7a264487823868fd2638094c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_280bb778d3734439966ebdee8629e860",
              "IPY_MODEL_64c27a81f92e453aafa69dc707544f64",
              "IPY_MODEL_1cc8115359cc45f4bb865f226165d0f6"
            ],
            "layout": "IPY_MODEL_79d7b2326b39434bac2deb1c607387d6"
          }
        },
        "280bb778d3734439966ebdee8629e860": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a3f494ff18c24160abbe8266c4d68227",
            "placeholder": "​",
            "style": "IPY_MODEL_afdf0c932e594423bd7d37163d72abc1",
            "value": "100%"
          }
        },
        "64c27a81f92e453aafa69dc707544f64": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0085b2ad4e7547c09f961d43a210fcb8",
            "max": 170498071,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1dc6905e6e1b4c96a7d8f23ffb5e9745",
            "value": 170498071
          }
        },
        "1cc8115359cc45f4bb865f226165d0f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c8eeee2beda94686801adc4b42f98bd8",
            "placeholder": "​",
            "style": "IPY_MODEL_ba454c2789714ce386ba47b5638e7755",
            "value": " 170498071/170498071 [00:19&lt;00:00, 7688599.00it/s]"
          }
        },
        "79d7b2326b39434bac2deb1c607387d6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a3f494ff18c24160abbe8266c4d68227": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "afdf0c932e594423bd7d37163d72abc1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0085b2ad4e7547c09f961d43a210fcb8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1dc6905e6e1b4c96a7d8f23ffb5e9745": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c8eeee2beda94686801adc4b42f98bd8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ba454c2789714ce386ba47b5638e7755": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "85156346d92b4be3a44649916cd0a1d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_eebf649478044f95bdcea51390a1029d",
              "IPY_MODEL_1baef2be3f0a4862b5a4775c4cf0da23",
              "IPY_MODEL_46c50a194c2144dc96f16246d493d6ef"
            ],
            "layout": "IPY_MODEL_fdb4c5f03e7d4ce58d5c6e1c54567bb4"
          }
        },
        "eebf649478044f95bdcea51390a1029d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c4d8ed0339c0487ba12f9cccfe125eea",
            "placeholder": "​",
            "style": "IPY_MODEL_641c08352b974e32beb3916ef57fd265",
            "value": "Epoch 0: 100%"
          }
        },
        "1baef2be3f0a4862b5a4775c4cf0da23": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_948e26a6eefc4a348b164f04fe0404ea",
            "max": 1563,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c05cb1ea2ef84fab8ee54bd53ff289c2",
            "value": 1563
          }
        },
        "46c50a194c2144dc96f16246d493d6ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f4bab902fb904b1788dd4be2bc2ef8fc",
            "placeholder": "​",
            "style": "IPY_MODEL_6bef34b9c6cb48daa04963dea7da8d3a",
            "value": " 1563/1563 [08:29&lt;00:00,  3.54batch/s, loss=0.237]"
          }
        },
        "fdb4c5f03e7d4ce58d5c6e1c54567bb4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c4d8ed0339c0487ba12f9cccfe125eea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "641c08352b974e32beb3916ef57fd265": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "948e26a6eefc4a348b164f04fe0404ea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c05cb1ea2ef84fab8ee54bd53ff289c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f4bab902fb904b1788dd4be2bc2ef8fc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6bef34b9c6cb48daa04963dea7da8d3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cohitai/AI_notebooks/blob/main/VAE/VAE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Variational AutoEncoder = (encoder= Recognition model, decoder=Generative model)+ Variational loss.\n",
        "\n",
        " Variational loss = ELBO (evidence lower bound).\n",
        "\n"
      ],
      "metadata": {
        "id": "TjIgUpV0H-Nq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ToDO: visualizations of the latent space, KL divergence explanations, Generative data. \n",
        "\n",
        "# Go over these: \n",
        "\n",
        "# https://github.com/AntixK/PyTorch-VAE (many different types of VAEs)\n",
        "# https://www.jeremyjordan.me/variational-autoencoders/\n",
        "# https://gaussian37.github.io/deep-learning-chollet-8-4/\n",
        "# https://medium.com/@arjun.majumdar/variational-autoencoder-cifar-10-tf2-9ed1155771e1\n",
        "# https://github.com/arjun-majumdar/Autoencoders_Experiments/blob/master/Variational_AutoEncoder_CIFAR10_TF2.ipynb\n",
        "# https://towardsdatascience.com/variational-autoencoder-demystified-with-pytorch-implementation-3a06bee395ed\n",
        "# https://colab.research.google.com/drive/1_yGmk8ahWhDs23U4mpplBFa-39fsEJoT?usp=sharing#scrollTo=xOVursP7lpR-\n",
        "\n",
        "# More ToDo's:\n",
        "# 1. plot the losses.\n",
        "# 2. plot reconstruction versus original images as 2 X M subplot. \n",
        "# 2. Visualization of the latent space from https://avandekleut.github.io/vae/\n",
        "# 3. evaluation data + some kind of accuracy\n",
        "# 4. sum() or mean() in loss , and kl.sum() (probabely) or kl.mean() ?  (OK)\n",
        "# 5. Compare the loss with the pretrained VAE model(Huggingfaces). \n",
        "\n",
        "# Improvements\n",
        "# 0. adjust better the learning rate\n",
        "# 1. Exponential Learning rate ? shadow variables etc \n",
        "# 2. Perceptual Losses for Deep Image Restoration\n",
        "# 3. https://towardsdatascience.com/perceptual-losses-for-image-restoration-dd3c9de4113\n",
        "# 4. Compare the loss with the pretrained VAE model(Huggingfaces). \n",
        "\n",
        "\n",
        "# Theory: main paper on VAE: (Auto-Encoding Variational Bayes) at https://arxiv.org/pdf/1312.6114.pdf\n",
        "\n",
        "# Unet variations\n",
        "# Unet: https://amaarora.github.io/2020/09/13/unet.html\n",
        "# Unet paper: U-Net: Convolutional Networks for Biomedical Image Segmentation (https://arxiv.org/pdf/1505.04597.pdf)\n",
        "\n",
        "# Diffusers on local M1 ANE (transfer to Colab). "
      ],
      "metadata": {
        "id": "VrgX3VIFo8Dk"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch \n",
        "import torchvision\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import os\n",
        "import math\n",
        "\n",
        "print(\"Working directory:\", os.getcwd())\n",
        "device=\"cuda\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rMwmbbQMWjkm",
        "outputId": "151c8f91-b644-480f-9764-bc98fdf4faa1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Working directory: /content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d-ZChfoGGKpW",
        "outputId": "babc7690-b704-409e-f75b-7f41ebcedbea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Nov 15 12:14:33 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   39C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Blocks:\n",
        " - DownEncoderBlock2D\n",
        "\n",
        " - UpDecoderBlock2D\n",
        "\n",
        " - ResnetBlock\n",
        "\n",
        " - Downsample2D\n",
        "\n",
        " - Upsample2D\n",
        "\n",
        " - AttentionBlockNew\n",
        "\n",
        " - UNetMidBlock2D\n",
        "\n",
        " - Encoder\n",
        "\n",
        " - Decoder\n"
      ],
      "metadata": {
        "id": "JZ3V7x4oWwCJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DownEncoderBlock2D(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels: int,\n",
        "        out_channels: int,\n",
        "        dropout: float = 0.0, \n",
        "        num_layers: int = 1,\n",
        "        resnet_eps: float = 1e-6,\n",
        "        resnet_time_scale_shift: str = \"default\",\n",
        "        resnet_act_fn: str = \"swish\",\n",
        "        resnet_groups: int = 32,\n",
        "        resnet_pre_norm: bool = True,\n",
        "        output_scale_factor=1.0,\n",
        "        add_downsample=True,\n",
        "        downsample_padding=1,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        resnets = []\n",
        "\n",
        "        for i in range(num_layers):\n",
        "            in_channels = in_channels if i == 0 else out_channels\n",
        "            resnets.append(\n",
        "                ResnetBlock(\n",
        "                    in_channels=in_channels,\n",
        "                    out_channels=out_channels,\n",
        "                    temb_channels=None,\n",
        "                    eps=resnet_eps,\n",
        "                    groups=resnet_groups,\n",
        "                    dropout=dropout,\n",
        "                    time_embedding_norm=resnet_time_scale_shift,\n",
        "                    non_linearity=resnet_act_fn,\n",
        "                    output_scale_factor=output_scale_factor,\n",
        "                    pre_norm=resnet_pre_norm,\n",
        "                )\n",
        "            )\n",
        "\n",
        "        self.resnets = nn.ModuleList(resnets)\n",
        "\n",
        "        if add_downsample:\n",
        "            self.downsamplers = nn.ModuleList(\n",
        "                [\n",
        "                    Downsample2D(\n",
        "                        in_channels, use_conv=True, out_channels=out_channels, padding=downsample_padding, name=\"op\"\n",
        "                    )\n",
        "                ]\n",
        "            )\n",
        "        else:\n",
        "            self.downsamplers = None\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        for resnet in self.resnets:\n",
        "            hidden_states = resnet(hidden_states, temb=None)\n",
        "\n",
        "        if self.downsamplers is not None:\n",
        "            for downsampler in self.downsamplers:\n",
        "                hidden_states = downsampler(hidden_states)\n",
        "\n",
        "        return hidden_states\n",
        "\n",
        "class UpDecoderBlock2D(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels: int,\n",
        "        out_channels: int,\n",
        "        dropout: float = 0.0,\n",
        "        num_layers: int = 1,\n",
        "        resnet_eps: float = 1e-6,\n",
        "        resnet_time_scale_shift: str = \"default\",\n",
        "        resnet_act_fn: str = \"swish\",\n",
        "        resnet_groups: int = 32,\n",
        "        resnet_pre_norm: bool = True,\n",
        "        output_scale_factor=1.0,\n",
        "        add_upsample=True,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        resnets = []\n",
        "\n",
        "        for i in range(num_layers):\n",
        "            input_channels = in_channels if i == 0 else out_channels\n",
        "\n",
        "            resnets.append(\n",
        "                ResnetBlock(\n",
        "                    in_channels=input_channels,\n",
        "                    out_channels=out_channels,\n",
        "                    temb_channels=None,\n",
        "                    eps=resnet_eps,\n",
        "                    groups=resnet_groups,\n",
        "                    dropout=dropout,\n",
        "                    time_embedding_norm=resnet_time_scale_shift,\n",
        "                    non_linearity=resnet_act_fn,\n",
        "                    output_scale_factor=output_scale_factor,\n",
        "                    pre_norm=resnet_pre_norm,\n",
        "                )\n",
        "            )\n",
        "\n",
        "        self.resnets = nn.ModuleList(resnets)\n",
        "\n",
        "        if add_upsample:\n",
        "            self.upsamplers = nn.ModuleList([Upsample2D(out_channels, use_conv=True, out_channels=out_channels)])\n",
        "        else:\n",
        "            self.upsamplers = None\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        for resnet in self.resnets:\n",
        "            hidden_states = resnet(hidden_states, temb=None)\n",
        "\n",
        "        if self.upsamplers is not None:\n",
        "            for upsampler in self.upsamplers:\n",
        "                hidden_states = upsampler(hidden_states)\n",
        "\n",
        "        return hidden_states\n",
        "\n",
        "  \n",
        "    \n",
        "class ResnetBlock(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        *,\n",
        "        in_channels,\n",
        "        out_channels=None,\n",
        "        conv_shortcut=False,\n",
        "        dropout=0.0,\n",
        "        temb_channels=512,\n",
        "        groups=32,\n",
        "        groups_out=None,\n",
        "        pre_norm=True,\n",
        "        eps=1e-6,\n",
        "        non_linearity=\"swish\",\n",
        "        time_embedding_norm=\"default\",\n",
        "        kernel=None,\n",
        "        output_scale_factor=1.0,\n",
        "        use_nin_shortcut=None,\n",
        "        up=False,\n",
        "        down=False,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.pre_norm = pre_norm\n",
        "        self.pre_norm = True\n",
        "        self.in_channels = in_channels\n",
        "        out_channels = in_channels if out_channels is None else out_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.use_conv_shortcut = conv_shortcut\n",
        "        self.time_embedding_norm = time_embedding_norm\n",
        "        self.up = up\n",
        "        self.down = down\n",
        "        self.output_scale_factor = output_scale_factor\n",
        "\n",
        "        if groups_out is None:\n",
        "            groups_out = groups\n",
        "\n",
        "        self.norm1 = torch.nn.GroupNorm(num_groups=groups, num_channels=in_channels, eps=eps, affine=True)\n",
        "\n",
        "        self.conv1 = torch.nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "        if temb_channels is not None:\n",
        "            self.time_emb_proj = torch.nn.Linear(temb_channels, out_channels)\n",
        "        else:\n",
        "            self.time_emb_proj = None\n",
        "\n",
        "        self.norm2 = torch.nn.GroupNorm(num_groups=groups_out, num_channels=out_channels, eps=eps, affine=True)\n",
        "        self.dropout = torch.nn.Dropout(dropout)\n",
        "        self.conv2 = torch.nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "        if non_linearity == \"swish\":\n",
        "            self.nonlinearity = lambda x: F.silu(x)\n",
        "        elif non_linearity == \"mish\":\n",
        "            self.nonlinearity = Mish()\n",
        "        elif non_linearity == \"silu\":\n",
        "            self.nonlinearity = nn.SiLU()\n",
        "\n",
        "        self.upsample = self.downsample = None\n",
        "        if self.up:\n",
        "            if kernel == \"fir\":\n",
        "                fir_kernel = (1, 3, 3, 1)\n",
        "                self.upsample = lambda x: upsample_2d(x, k=fir_kernel)\n",
        "            elif kernel == \"sde_vp\":\n",
        "                self.upsample = partial(F.interpolate, scale_factor=2.0, mode=\"nearest\")\n",
        "            else:\n",
        "                self.upsample = Upsample2D(in_channels, use_conv=False)\n",
        "        \n",
        "        elif self.down:\n",
        "            if kernel == \"fir\":\n",
        "                fir_kernel = (1, 3, 3, 1)\n",
        "                self.downsample = lambda x: downsample_2d(x, k=fir_kernel)\n",
        "            elif kernel == \"sde_vp\":\n",
        "                self.downsample = partial(F.avg_pool2d, kernel_size=2, stride=2)\n",
        "            else:\n",
        "                self.downsample = Downsample2D(in_channels, use_conv=False, padding=1, name=\"op\")\n",
        "\n",
        "        self.use_nin_shortcut = self.in_channels != self.out_channels if use_nin_shortcut is None else use_nin_shortcut\n",
        "\n",
        "        self.conv_shortcut = None\n",
        "    \n",
        "        if self.use_nin_shortcut:\n",
        "            self.conv_shortcut = torch.nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0)\n",
        "\n",
        "    def forward(self, x, temb, hey=False):\n",
        "        h = x\n",
        "\n",
        "        # print(\"LOG resnet_input\",h.size())\n",
        "\n",
        "        # make sure hidden states is in float32\n",
        "        # when running in half-precision\n",
        "        h = self.norm1(h.float()).type(h.dtype)\n",
        "        h = self.nonlinearity(h)\n",
        "\n",
        "        # if self.upsample is not None:\n",
        "        #     x = self.upsample(x)\n",
        "        #     h = self.upsample(h)\n",
        "        # elif self.downsample is not None:\n",
        "        #     x = self.downsample(x)\n",
        "        #     h = self.downsample(h)\n",
        "\n",
        "        h = self.conv1(h)\n",
        "\n",
        "        # print(\"LOG temb:\",temb is None)\n",
        "\n",
        "        # if temb is not None:\n",
        "        #     temb = self.time_emb_proj(self.nonlinearity(temb))[:, :, None, None]\n",
        "        #     h = h + temb\n",
        "\n",
        "        # make sure hidden states is in float32\n",
        "        # when running in half-precision\n",
        "        h = self.norm2(h.float()).type(h.dtype)\n",
        "        h = self.nonlinearity(h)\n",
        "\n",
        "        h = self.dropout(h)\n",
        "        h = self.conv2(h)\n",
        "\n",
        "        if self.conv_shortcut is not None:\n",
        "            x = self.conv_shortcut(x)\n",
        "\n",
        "        out = (x + h) / self.output_scale_factor\n",
        "\n",
        "        return out\n",
        "\n",
        "    def set_weight(self, resnet):\n",
        "        self.norm1.weight.data = resnet.norm1.weight.data\n",
        "        self.norm1.bias.data = resnet.norm1.bias.data\n",
        "\n",
        "        self.conv1.weight.data = resnet.conv1.weight.data\n",
        "        self.conv1.bias.data = resnet.conv1.bias.data\n",
        "\n",
        "        if self.time_emb_proj is not None:\n",
        "            self.time_emb_proj.weight.data = resnet.temb_proj.weight.data\n",
        "            self.time_emb_proj.bias.data = resnet.temb_proj.bias.data\n",
        "\n",
        "        self.norm2.weight.data = resnet.norm2.weight.data\n",
        "        self.norm2.bias.data = resnet.norm2.bias.data\n",
        "\n",
        "        self.conv2.weight.data = resnet.conv2.weight.data\n",
        "        self.conv2.bias.data = resnet.conv2.bias.data\n",
        "\n",
        "        if self.use_nin_shortcut:\n",
        "            self.conv_shortcut.weight.data = resnet.nin_shortcut.weight.data\n",
        "            self.conv_shortcut.bias.data = resnet.nin_shortcut.bias.data\n",
        "            \n",
        "            \n",
        "class Downsample2D(nn.Module):\n",
        "    \"\"\"\n",
        "    A downsampling layer with an optional convolution.\n",
        "    :param channels: channels in the inputs and outputs. :param use_conv: a bool determining if a convolution is\n",
        "    applied. :param dims: determines if the signal is 1D, 2D, or 3D. If 3D, then\n",
        "                 downsampling occurs in the inner-two dimensions.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, channels, use_conv=False, out_channels=None, padding=1, name=\"conv\"):\n",
        "        super().__init__()\n",
        "        self.channels = channels\n",
        "        self.out_channels = out_channels or channels\n",
        "        self.use_conv = use_conv\n",
        "        self.padding = padding\n",
        "        stride = 2\n",
        "        self.name = name\n",
        "\n",
        "        if use_conv:\n",
        "            conv = nn.Conv2d(self.channels, self.out_channels, 3, stride=stride, padding=padding)\n",
        "        else:\n",
        "            assert self.channels == self.out_channels\n",
        "            conv = nn.AvgPool2d(kernel_size=stride, stride=stride)\n",
        "\n",
        "        # TODO(Suraj, Patrick) - clean up after weight dicts are correctly renamed\n",
        "        if name == \"conv\":\n",
        "            self.Conv2d_0 = conv\n",
        "            self.conv = conv\n",
        "        elif name == \"Conv2d_0\":\n",
        "            self.conv = conv\n",
        "        else:\n",
        "            self.conv = conv\n",
        "\n",
        "    def forward(self, x):\n",
        "        assert x.shape[1] == self.channels\n",
        "        if self.use_conv and self.padding == 0:\n",
        "            pad = (0, 1, 0, 1)\n",
        "            x = F.pad(x, pad, mode=\"constant\", value=0)\n",
        "\n",
        "        assert x.shape[1] == self.channels\n",
        "        x = self.conv(x)\n",
        "\n",
        "        # print('LOG',x)\n",
        "        return x\n",
        "\n",
        "class Upsample2D(nn.Module):\n",
        "\n",
        "    \"\"\"\n",
        "    An upsampling layer with an optional convolution.\n",
        "    :param channels: channels in the inputs and outputs. :param use_conv: a bool determining if a convolution is\n",
        "    applied. :param dims: determines if the signal is 1D, 2D, or 3D. If 3D, then\n",
        "                 upsampling occurs in the inner-two dimensions.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, channels, use_conv=False, use_conv_transpose=False, out_channels=None, name=\"conv\"):\n",
        "        super().__init__()\n",
        "        self.channels = channels\n",
        "        self.out_channels = out_channels or channels\n",
        "        self.use_conv = use_conv\n",
        "        self.use_conv_transpose = use_conv_transpose\n",
        "        self.name = name\n",
        "\n",
        "        conv = None\n",
        "        if use_conv_transpose:\n",
        "            conv = nn.ConvTranspose2d(channels, self.out_channels, 4, 2, 1)\n",
        "        elif use_conv:\n",
        "            conv = nn.Conv2d(self.channels, self.out_channels, 3, padding=1)\n",
        "\n",
        "        # TODO(Suraj, Patrick) - clean up after weight dicts are correctly renamed\n",
        "        if name == \"conv\":\n",
        "            self.conv = conv\n",
        "        else:\n",
        "            self.Conv2d_0 = conv\n",
        "\n",
        "    def forward(self, x):\n",
        "        assert x.shape[1] == self.channels\n",
        "        if self.use_conv_transpose:\n",
        "            return self.conv(x)\n",
        "\n",
        "        x = F.interpolate(x, scale_factor=2.0, mode=\"nearest\")\n",
        "\n",
        "        # TODO(Suraj, Patrick) - clean up after weight dicts are correctly renamed\n",
        "        if self.use_conv:\n",
        "            if self.name == \"conv\":\n",
        "                x = self.conv(x)\n",
        "            else:\n",
        "                x = self.Conv2d_0(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class AttentionBlockNew(nn.Module):\n",
        "    \"\"\"\n",
        "    An attention block that allows spatial positions to attend to each other. Originally ported from here, but adapted\n",
        "    to the N-d case.\n",
        "    https://github.com/hojonathanho/diffusion/blob/1e0dceb3b3495bbe19116a5e1b3596cd0706c543/diffusion_tf/models/unet.py#L66.\n",
        "    Uses three q, k, v linear layers to compute attention\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        channels,\n",
        "        num_head_channels=None,\n",
        "        num_groups=32,\n",
        "        rescale_output_factor=1.0,\n",
        "        eps=1e-5,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.channels = channels\n",
        "\n",
        "        self.num_heads = channels // num_head_channels if num_head_channels is not None else 1\n",
        "        self.num_head_size = num_head_channels\n",
        "        self.group_norm = nn.GroupNorm(num_channels=channels, num_groups=num_groups, eps=eps, affine=True)\n",
        "\n",
        "        # define q,k,v as linear layers\n",
        "        self.query = nn.Linear(channels, channels)\n",
        "        self.key = nn.Linear(channels, channels)\n",
        "        self.value = nn.Linear(channels, channels)\n",
        "\n",
        "        self.rescale_output_factor = rescale_output_factor\n",
        "        self.proj_attn = nn.Linear(channels, channels, 1)\n",
        "\n",
        "    def transpose_for_scores(self, projection: torch.Tensor) -> torch.Tensor:\n",
        "        new_projection_shape = projection.size()[:-1] + (self.num_heads, -1)\n",
        "        # move heads to 2nd position (B, T, H * D) -> (B, T, H, D) -> (B, H, T, D)\n",
        "        new_projection = projection.view(new_projection_shape).permute(0, 2, 1, 3)\n",
        "        return new_projection\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        residual = hidden_states\n",
        "        batch, channel, height, width = hidden_states.shape\n",
        "\n",
        "        # norm\n",
        "        hidden_states = self.group_norm(hidden_states)\n",
        "\n",
        "        hidden_states = hidden_states.view(batch, channel, height * width).transpose(1, 2)\n",
        "\n",
        "        # proj to q, k, v\n",
        "        query_proj = self.query(hidden_states)\n",
        "        key_proj = self.key(hidden_states)\n",
        "        value_proj = self.value(hidden_states)\n",
        "\n",
        "        # transpose\n",
        "        query_states = self.transpose_for_scores(query_proj)\n",
        "        key_states = self.transpose_for_scores(key_proj)\n",
        "        value_states = self.transpose_for_scores(value_proj)\n",
        "\n",
        "        # get scores\n",
        "        scale = 1 / math.sqrt(math.sqrt(self.channels / self.num_heads))\n",
        "        attention_scores = torch.matmul(query_states * scale, key_states.transpose(-1, -2) * scale)\n",
        "        attention_probs = torch.softmax(attention_scores.float(), dim=-1).type(attention_scores.dtype)\n",
        "\n",
        "        # compute attention output\n",
        "        context_states = torch.matmul(attention_probs, value_states)\n",
        "\n",
        "        context_states = context_states.permute(0, 2, 1, 3).contiguous()\n",
        "        new_context_states_shape = context_states.size()[:-2] + (self.channels,)\n",
        "        context_states = context_states.view(new_context_states_shape)\n",
        "\n",
        "        # compute next hidden_states\n",
        "        hidden_states = self.proj_attn(context_states)\n",
        "        hidden_states = hidden_states.transpose(-1, -2).reshape(batch, channel, height, width)\n",
        "\n",
        "        # res connect and rescale\n",
        "        hidden_states = (hidden_states + residual) / self.rescale_output_factor\n",
        "        return hidden_states\n",
        "\n",
        "    def set_weight(self, attn_layer):\n",
        "        self.group_norm.weight.data = attn_layer.norm.weight.data\n",
        "        self.group_norm.bias.data = attn_layer.norm.bias.data\n",
        "\n",
        "        if hasattr(attn_layer, \"q\"):\n",
        "            self.query.weight.data = attn_layer.q.weight.data[:, :, 0, 0]\n",
        "            self.key.weight.data = attn_layer.k.weight.data[:, :, 0, 0]\n",
        "            self.value.weight.data = attn_layer.v.weight.data[:, :, 0, 0]\n",
        "\n",
        "            self.query.bias.data = attn_layer.q.bias.data\n",
        "            self.key.bias.data = attn_layer.k.bias.data\n",
        "            self.value.bias.data = attn_layer.v.bias.data\n",
        "\n",
        "            self.proj_attn.weight.data = attn_layer.proj_out.weight.data[:, :, 0, 0]\n",
        "            self.proj_attn.bias.data = attn_layer.proj_out.bias.data\n",
        "        elif hasattr(attn_layer, \"NIN_0\"):\n",
        "            self.query.weight.data = attn_layer.NIN_0.W.data.T\n",
        "            self.key.weight.data = attn_layer.NIN_1.W.data.T\n",
        "            self.value.weight.data = attn_layer.NIN_2.W.data.T\n",
        "\n",
        "            self.query.bias.data = attn_layer.NIN_0.b.data\n",
        "            self.key.bias.data = attn_layer.NIN_1.b.data\n",
        "            self.value.bias.data = attn_layer.NIN_2.b.data\n",
        "\n",
        "            self.proj_attn.weight.data = attn_layer.NIN_3.W.data.T\n",
        "            self.proj_attn.bias.data = attn_layer.NIN_3.b.data\n",
        "\n",
        "            self.group_norm.weight.data = attn_layer.GroupNorm_0.weight.data\n",
        "            self.group_norm.bias.data = attn_layer.GroupNorm_0.bias.data\n",
        "        else:\n",
        "            qkv_weight = attn_layer.qkv.weight.data.reshape(\n",
        "                self.num_heads, 3 * self.channels // self.num_heads, self.channels\n",
        "            )\n",
        "            qkv_bias = attn_layer.qkv.bias.data.reshape(self.num_heads, 3 * self.channels // self.num_heads)\n",
        "\n",
        "            q_w, k_w, v_w = qkv_weight.split(self.channels // self.num_heads, dim=1)\n",
        "            q_b, k_b, v_b = qkv_bias.split(self.channels // self.num_heads, dim=1)\n",
        "\n",
        "            self.query.weight.data = q_w.reshape(-1, self.channels)\n",
        "            self.key.weight.data = k_w.reshape(-1, self.channels)\n",
        "            self.value.weight.data = v_w.reshape(-1, self.channels)\n",
        "\n",
        "            self.query.bias.data = q_b.reshape(-1)\n",
        "            self.key.bias.data = k_b.reshape(-1)\n",
        "            self.value.bias.data = v_b.reshape(-1)\n",
        "\n",
        "            self.proj_attn.weight.data = attn_layer.proj.weight.data[:, :, 0]\n",
        "            self.proj_attn.bias.data = attn_layer.proj.bias.data\n",
        "\n",
        "class UNetMidBlock2D(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels: int,\n",
        "        temb_channels: int,\n",
        "        dropout: float = 0.0,\n",
        "        num_layers: int = 1,\n",
        "        resnet_eps: float = 1e-6,\n",
        "        resnet_time_scale_shift: str = \"default\",\n",
        "        resnet_act_fn: str = \"swish\",\n",
        "        resnet_groups: int = 32,\n",
        "        resnet_pre_norm: bool = True,\n",
        "        attn_num_head_channels=1,\n",
        "        attention_type=\"default\",\n",
        "        output_scale_factor=1.0,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.attention_type = attention_type\n",
        "        resnet_groups = resnet_groups if resnet_groups is not None else min(in_channels // 4, 32)\n",
        "\n",
        "        # there is always at least one resnet\n",
        "        resnets = [\n",
        "            ResnetBlock(\n",
        "                in_channels=in_channels,\n",
        "                out_channels=in_channels,\n",
        "                temb_channels=temb_channels,\n",
        "                eps=resnet_eps,\n",
        "                groups=resnet_groups,\n",
        "                dropout=dropout,\n",
        "                time_embedding_norm=resnet_time_scale_shift,\n",
        "                non_linearity=resnet_act_fn,\n",
        "                output_scale_factor=output_scale_factor,\n",
        "                pre_norm=resnet_pre_norm,\n",
        "            )\n",
        "        ]\n",
        "        attentions = []\n",
        "\n",
        "        for _ in range(num_layers):\n",
        "            attentions.append(\n",
        "                AttentionBlockNew(\n",
        "                    in_channels,\n",
        "                    num_head_channels=attn_num_head_channels,\n",
        "                    rescale_output_factor=output_scale_factor,\n",
        "                    eps=resnet_eps,\n",
        "                    num_groups=resnet_groups,\n",
        "                )\n",
        "            )\n",
        "            resnets.append(\n",
        "                ResnetBlock(\n",
        "                    in_channels=in_channels,\n",
        "                    out_channels=in_channels,\n",
        "                    temb_channels=temb_channels,\n",
        "                    eps=resnet_eps,\n",
        "                    groups=resnet_groups,\n",
        "                    dropout=dropout,\n",
        "                    time_embedding_norm=resnet_time_scale_shift,\n",
        "                    non_linearity=resnet_act_fn,\n",
        "                    output_scale_factor=output_scale_factor,\n",
        "                    pre_norm=resnet_pre_norm,\n",
        "                )\n",
        "            )\n",
        "\n",
        "        self.attentions = nn.ModuleList(attentions)\n",
        "        self.resnets = nn.ModuleList(resnets)\n",
        "\n",
        "    def forward(self, hidden_states, temb=None, encoder_states=None):\n",
        "        hidden_states = self.resnets[0](hidden_states, temb)\n",
        "        for attn, resnet in zip(self.attentions, self.resnets[1:]):\n",
        "            if self.attention_type == \"default\":\n",
        "                hidden_states = attn(hidden_states)\n",
        "            else:\n",
        "                hidden_states = attn(hidden_states, encoder_states)\n",
        "            hidden_states = resnet(hidden_states, temb)\n",
        "\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels=3,\n",
        "        out_channels=4,\n",
        "        down_block_types=(\"DownEncoderBlock2D\",),\n",
        "        block_out_channels=(64,),\n",
        "        layers_per_block=1,\n",
        "        act_fn=\"silu\",\n",
        "        double_z=True,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.layers_per_block = layers_per_block\n",
        "        self.conv_in = torch.nn.Conv2d(in_channels, block_out_channels[0], kernel_size=3, stride=1, padding=1)\n",
        "        self.mid_block = None\n",
        "        self.down_blocks = nn.ModuleList([])\n",
        "\n",
        "        # down\n",
        "        output_channel = block_out_channels[0]\n",
        "        for i, _ in enumerate(down_block_types):\n",
        "            input_channel = output_channel\n",
        "            output_channel = block_out_channels[i]\n",
        "            is_final_block = i == len(block_out_channels) - 1\n",
        "\n",
        "            down_block = DownEncoderBlock2D(\n",
        "                num_layers=self.layers_per_block,\n",
        "                in_channels=input_channel,\n",
        "                out_channels=output_channel,\n",
        "                add_downsample=not is_final_block,\n",
        "                resnet_eps=1e-6,\n",
        "                resnet_act_fn=act_fn,\n",
        "                downsample_padding=0,)\n",
        "\n",
        "            self.down_blocks.append(down_block)\n",
        "\n",
        "        # mid\n",
        "        self.mid_block = UNetMidBlock2D(\n",
        "            in_channels=block_out_channels[-1],\n",
        "            resnet_eps=1e-6,\n",
        "            resnet_act_fn=act_fn,\n",
        "            output_scale_factor=1,\n",
        "            resnet_time_scale_shift=\"default\",\n",
        "            attn_num_head_channels=None,\n",
        "            resnet_groups=32,\n",
        "            temb_channels=None,\n",
        "        )\n",
        "\n",
        "        # out\n",
        "        num_groups_out = 32\n",
        "        self.conv_norm_out = nn.GroupNorm(num_channels=block_out_channels[-1], num_groups=num_groups_out, eps=1e-6)\n",
        "        self.conv_act = nn.SiLU()\n",
        "\n",
        "        conv_out_channels = 2 * out_channels if double_z else out_channels\n",
        "        self.conv_out = nn.Conv2d(block_out_channels[-1], conv_out_channels, 3, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \n",
        "        sample = x\n",
        "        sample = self.conv_in(sample)\n",
        "        \n",
        "        # down\n",
        "        for down_block in self.down_blocks:\n",
        "            sample = down_block(sample)\n",
        "\n",
        "        # middle\n",
        "        sample = self.mid_block(sample)\n",
        "\n",
        "        # post-process\n",
        "        sample = self.conv_norm_out(sample)\n",
        "        sample = self.conv_act(sample)\n",
        "        sample = self.conv_out(sample)\n",
        "\n",
        "        return sample\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels=3,\n",
        "        out_channels=3,\n",
        "        up_block_types=(\"UpDecoderBlock2D\",),\n",
        "        block_out_channels=(64,),\n",
        "        layers_per_block=2,\n",
        "        act_fn=\"silu\",\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.layers_per_block = layers_per_block\n",
        "\n",
        "        self.conv_in = nn.Conv2d(in_channels, block_out_channels[-1], kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "        self.mid_block = None\n",
        "        self.up_blocks = nn.ModuleList([])\n",
        "\n",
        "        # mid\n",
        "        self.mid_block = UNetMidBlock2D(\n",
        "            in_channels=block_out_channels[-1],\n",
        "            resnet_eps=1e-6,\n",
        "            resnet_act_fn=act_fn,\n",
        "            output_scale_factor=1,\n",
        "            resnet_time_scale_shift=\"default\",\n",
        "            attn_num_head_channels=None,\n",
        "            resnet_groups=32,\n",
        "            temb_channels=None,\n",
        "        )\n",
        "\n",
        "        # up\n",
        "        reversed_block_out_channels = list(reversed(block_out_channels))\n",
        "        output_channel = reversed_block_out_channels[0]\n",
        "        for i, up_block_type in enumerate(up_block_types):\n",
        "            prev_output_channel = output_channel\n",
        "            output_channel = reversed_block_out_channels[i]\n",
        "\n",
        "            is_final_block = i == len(block_out_channels) - 1\n",
        "\n",
        "            up_block = UpDecoderBlock2D(\n",
        "            num_layers=self.layers_per_block + 1,\n",
        "            in_channels=prev_output_channel,\n",
        "            out_channels=output_channel,\n",
        "            add_upsample=not is_final_block,\n",
        "            resnet_eps=1e-6,\n",
        "            resnet_act_fn=act_fn,\n",
        "        )\n",
        "\n",
        "            self.up_blocks.append(up_block)\n",
        "            prev_output_channel = output_channel\n",
        "\n",
        "        # out\n",
        "        num_groups_out = 32\n",
        "        self.conv_norm_out = nn.GroupNorm(num_channels=block_out_channels[0], num_groups=num_groups_out, eps=1e-6)\n",
        "        self.conv_act = nn.SiLU()\n",
        "        self.conv_out = nn.Conv2d(block_out_channels[0], out_channels, 3, padding=1)\n",
        "\n",
        "    def forward(self, z):\n",
        "        sample = z\n",
        "        sample = self.conv_in(sample)\n",
        "\n",
        "        # middle\n",
        "        sample = self.mid_block(sample)\n",
        "\n",
        "        # up\n",
        "        for up_block in self.up_blocks:\n",
        "            sample = up_block(sample)\n",
        "\n",
        "        # post-process\n",
        "        sample = self.conv_norm_out(sample)\n",
        "        sample = self.conv_act(sample)\n",
        "        sample = self.conv_out(sample)\n",
        "\n",
        "        return sample"
      ],
      "metadata": {
        "id": "5_K2AE--WiX4"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DiagonalGaussianDistribution(object):\n",
        "    def __init__(self, parameters, deterministic=False):\n",
        "        self.parameters = parameters\n",
        "        self.mean, self.logvar = torch.chunk(parameters, 2, dim=1)\n",
        "        self.logvar = torch.clamp(self.logvar, -30.0, 20.0)\n",
        "        self.deterministic = deterministic\n",
        "        self.std = torch.exp(0.5 * self.logvar)\n",
        "        self.var = torch.exp(self.logvar)\n",
        "        if self.deterministic:\n",
        "            self.var = self.std = torch.zeros_like(self.mean).to(device=self.parameters.device)\n",
        "\n",
        "    def sample(self):\n",
        "        x = self.mean + self.std * torch.randn(self.mean.shape).to(device=self.parameters.device)\n",
        "        return x\n",
        "\n",
        "    def kl(self, other=None):\n",
        "        if self.deterministic:\n",
        "            return torch.Tensor([0.0])\n",
        "        else:\n",
        "            if other is None:\n",
        "                return 0.5 * torch.sum(torch.pow(self.mean, 2) + self.var - 1.0 - self.logvar, dim=[1, 2, 3])\n",
        "            else:\n",
        "                return 0.5 * torch.sum(\n",
        "                    torch.pow(self.mean - other.mean, 2) / other.var\n",
        "                    + self.var / other.var\n",
        "                    - 1.0\n",
        "                    - self.logvar\n",
        "                    + other.logvar,\n",
        "                    dim=[1, 2, 3],\n",
        "                )\n",
        "\n",
        "    def nll(self, sample, dims=[1, 2, 3]):\n",
        "        if self.deterministic:\n",
        "            return torch.Tensor([0.0])\n",
        "        logtwopi = np.log(2.0 * np.pi)\n",
        "        return 0.5 * torch.sum(logtwopi + self.logvar + torch.pow(sample - self.mean, 2) / self.var, dim=dims)\n",
        "\n",
        "    def mode(self):\n",
        "        return self.mean\n",
        "\n",
        "\n",
        "\n",
        "encoder = Encoder(in_channels=3,\n",
        "        out_channels=4,\n",
        "        down_block_types=(\"DownEncoderBlock2D\",),\n",
        "        block_out_channels=(64,),\n",
        "        layers_per_block=1,\n",
        "        act_fn=\"silu\",\n",
        "        double_z=True)\n",
        "\n",
        "\n",
        "decoder = Decoder(in_channels=4,\n",
        "            out_channels=3,\n",
        "            up_block_types=(\"UpDecoderBlock2D\",),\n",
        "            block_out_channels=(64,),\n",
        "            layers_per_block=1,\n",
        "            act_fn=\"silu\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    ##########################\n",
        "    ##########################\n",
        "    ##########################\n",
        "#### AutoEncoder wrap- up class ####\n",
        "    ##########################\n",
        "    ##########################\n",
        "    ##########################\n",
        "\n",
        "class AutoencoderKL(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels=3,\n",
        "        out_channels=3,\n",
        "        down_block_types=(\"DownEncoderBlock2D\",\"DownEncoderBlock2D\",\"DownEncoderBlock2D\",\"DownEncoderBlock2D\",),\n",
        "        up_block_types=(\"UpDecoderBlock2D\",\"UpDecoderBlock2D\",\"UpDecoderBlock2D\",\"UpDecoderBlock2D\",),\n",
        "        block_out_channels=(128, 256, 512, 512,),\n",
        "        layers_per_block=2,\n",
        "        act_fn=\"silu\",\n",
        "        latent_channels=4,\n",
        "        sample_size=512,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        # pass init params to Encoder\n",
        "        self.encoder = Encoder(\n",
        "            in_channels=in_channels,\n",
        "            out_channels=latent_channels,\n",
        "            down_block_types=down_block_types,\n",
        "            block_out_channels=block_out_channels,\n",
        "            layers_per_block=layers_per_block,\n",
        "            act_fn=act_fn,\n",
        "            double_z=True,\n",
        "        )\n",
        "\n",
        "        # pass init params to Decoder\n",
        "        self.decoder = Decoder(\n",
        "            in_channels=latent_channels,\n",
        "            out_channels=out_channels,\n",
        "            up_block_types=up_block_types,\n",
        "            block_out_channels=block_out_channels,\n",
        "            layers_per_block=layers_per_block,\n",
        "            act_fn=act_fn,\n",
        "        )\n",
        "\n",
        "        self.quant_conv = torch.nn.Conv2d(2 * latent_channels, 2 * latent_channels, 1)\n",
        "        self.post_quant_conv = torch.nn.Conv2d(latent_channels, latent_channels, 1) \n",
        "        self.kl = 0\n",
        "\n",
        "    def encode(self, x):\n",
        "        h = self.encoder(x)\n",
        "        moments = self.quant_conv(h)\n",
        "        posterior = DiagonalGaussianDistribution(moments)\n",
        "        return posterior\n",
        "\n",
        "    def decode(self, z):\n",
        "        z = self.post_quant_conv(z)\n",
        "        dec = self.decoder(z)\n",
        "        return dec\n",
        "\n",
        "    def forward(self, sample, sample_posterior=False):\n",
        "        x = sample\n",
        "        posterior = self.encode(x)\n",
        "        if sample_posterior:\n",
        "            z = posterior.sample()\n",
        "        else:\n",
        "            z = posterior.mode()\n",
        "        dec = self.decode(z)\n",
        "        self.kl=posterior.kl()\n",
        "        return dec\n"
      ],
      "metadata": {
        "id": "wpwvB6BhWvDF"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Training function**"
      ],
      "metadata": {
        "id": "crEhgj4FVZTB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##### train\n",
        "from tqdm.auto import tqdm\n",
        "PATH = './vae_net.pth' # default path\n",
        "losses=[]\n",
        "def train(autoencoder, data, epochs=20,exp=None):\n",
        "    \n",
        "    def adjust(sample):\n",
        "        # sample=np.expand_dims(np.asarray(sample), axis=0)\n",
        "        sample=sample.numpy()/ 1.0\n",
        "        # sample=sample.astype('float32')\n",
        "        sample = torch.from_numpy(sample).float() # Batch - RGB channel - WxH \n",
        "        sample = 2 * (sample - 0.5) # values between (-1, 1)\n",
        "        return sample\n",
        "\n",
        "    if exp:\n",
        "        model_file='./{}_net.pth'.format(exp)\n",
        "    else:\n",
        "        model_file=PATH\n",
        "    \n",
        "    global opt\n",
        "    opt = torch.optim.Adam(autoencoder.parameters())\n",
        "    scheduler = torch.optim.lr_scheduler.ExponentialLR(opt, gamma=0.9,verbose=True)\n",
        "    epoch = 0\n",
        "\n",
        "    if os.path.exists(model_file):\n",
        "      checkpoint = torch.load(model_file)\n",
        "      autoencoder.load_state_dict(checkpoint['model_state_dict'])\n",
        "      opt.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "      scheduler.load_state_dict(checkpoint['scheduler'])\n",
        "      epoch = checkpoint['epoch']\n",
        "      loss = checkpoint['loss']\n",
        "\n",
        "    for epoch in range(epoch, epochs):\n",
        "        # print(\"Log epoch:\",epoch)\n",
        "\n",
        "       with tqdm(data, unit=\"batch\",total=len(data)) as tepoch:\n",
        "        for i, x in enumerate(tepoch):\n",
        "            tepoch.set_description(f\"Epoch {epoch}\")\n",
        "\n",
        "        # for i,x in enumerate(tqdm(data,total=len(data))):\n",
        "            x=adjust(x[0]) # Values between -1..1 \n",
        "            x = torch.Tensor(x).to(device) # GPU\n",
        "            opt.zero_grad()\n",
        "            x_hat = autoencoder(x)\n",
        "            # loss = ((x - x_hat)**2).sum() + autoencoder.kl.sum()\n",
        "            loss = F.mse_loss(x,x_hat) + autoencoder.kl.mean()\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            loss=loss.item()\n",
        "            losses.append(loss)\n",
        "            tepoch.set_postfix(loss=loss)\n",
        "\n",
        "            # print(loss)\n",
        "\n",
        "            if not i % 300 and i != 0: \n",
        "                torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': autoencoder.state_dict(),\n",
        "                'optimizer_state_dict': opt.state_dict(),\n",
        "                'loss': loss,\n",
        "                'scheduler':scheduler.state_dict()}, model_file)\n",
        "                \n",
        "                print(\"model saved!\")\n",
        "                scheduler.step()\n",
        "        \n",
        "        torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': autoencoder.state_dict(),\n",
        "                'optimizer_state_dict': opt.state_dict(),\n",
        "                'loss': loss,\n",
        "                'scheduler':scheduler.state_dict()}, model_file)\n",
        "        print(\"model saved!\")\n",
        "   \n",
        "    return autoencoder"
      ],
      "metadata": {
        "id": "hieXAg7cVX-x"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**DataLoader Cifar10** "
      ],
      "metadata": {
        "id": "AKgZYAGTVpnx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ToDo: https://gist.github.com/kevinzakka/d33bf8d6c7f06a9d8c76d97a7879f5cb\n",
        "# normalize dataset.\n",
        "\n",
        "def load_data(BS=32,path = \"./cifar-10-batches-py\"):\n",
        "    train = torch.utils.data.DataLoader(\n",
        "        torchvision.datasets.CIFAR10(path,\n",
        "               transform=torchvision.transforms.ToTensor(),\n",
        "               download=True),\n",
        "        batch_size=32,\n",
        "        shuffle=True)\n",
        "    \n",
        "    test = torch.utils.data.DataLoader(\n",
        "        torchvision.datasets.CIFAR10(path,\n",
        "               transform=torchvision.transforms.ToTensor(),\n",
        "               download=True,train=False),\n",
        "        batch_size=32,\n",
        "        shuffle=True)\n",
        "        \n",
        "    print(train.dataset)\n",
        "    print(test.dataset)\n",
        "    print(\"train data size:\",next(iter(train))[0].size())\n",
        "\n",
        "    return train,test\n"
      ],
      "metadata": {
        "id": "0ZSbEP8ik-CG"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train/Test Split**\n"
      ],
      "metadata": {
        "id": "dVGV1ZUMj7Fg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data,test_data=load_data(BS=32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 355,
          "referenced_widgets": [
            "016e155f7a264487823868fd2638094c",
            "280bb778d3734439966ebdee8629e860",
            "64c27a81f92e453aafa69dc707544f64",
            "1cc8115359cc45f4bb865f226165d0f6",
            "79d7b2326b39434bac2deb1c607387d6",
            "a3f494ff18c24160abbe8266c4d68227",
            "afdf0c932e594423bd7d37163d72abc1",
            "0085b2ad4e7547c09f961d43a210fcb8",
            "1dc6905e6e1b4c96a7d8f23ffb5e9745",
            "c8eeee2beda94686801adc4b42f98bd8",
            "ba454c2789714ce386ba47b5638e7755"
          ]
        },
        "id": "OC6eRUf5kACL",
        "outputId": "7ff7de8d-2260-4490-c12b-38810e969125"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./cifar-10-batches-py/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/170498071 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "016e155f7a264487823868fd2638094c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./cifar-10-batches-py/cifar-10-python.tar.gz to ./cifar-10-batches-py\n",
            "Files already downloaded and verified\n",
            "Dataset CIFAR10\n",
            "    Number of datapoints: 50000\n",
            "    Root location: ./cifar-10-batches-py\n",
            "    Split: Train\n",
            "    StandardTransform\n",
            "Transform: ToTensor()\n",
            "Dataset CIFAR10\n",
            "    Number of datapoints: 10000\n",
            "    Root location: ./cifar-10-batches-py\n",
            "    Split: Test\n",
            "    StandardTransform\n",
            "Transform: ToTensor()\n",
            "train data size: torch.Size([32, 3, 32, 32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training Variational AutoEncoder on Cifar10 data."
      ],
      "metadata": {
        "id": "KWrWRwtFA6hh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data,test_data=load_data(BS=32)\n",
        "vae=AutoencoderKL().to(device)\n",
        "vae = train(vae, train_data,epochs=1,exp=\"test2\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 531,
          "referenced_widgets": [
            "85156346d92b4be3a44649916cd0a1d1",
            "eebf649478044f95bdcea51390a1029d",
            "1baef2be3f0a4862b5a4775c4cf0da23",
            "46c50a194c2144dc96f16246d493d6ef",
            "fdb4c5f03e7d4ce58d5c6e1c54567bb4",
            "c4d8ed0339c0487ba12f9cccfe125eea",
            "641c08352b974e32beb3916ef57fd265",
            "948e26a6eefc4a348b164f04fe0404ea",
            "c05cb1ea2ef84fab8ee54bd53ff289c2",
            "f4bab902fb904b1788dd4be2bc2ef8fc",
            "6bef34b9c6cb48daa04963dea7da8d3a"
          ]
        },
        "id": "sPWD7vMck51E",
        "outputId": "7cd54e3a-5697-4304-d262-6c4635aa7696"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Dataset CIFAR10\n",
            "    Number of datapoints: 50000\n",
            "    Root location: ./cifar-10-batches-py\n",
            "    Split: Train\n",
            "    StandardTransform\n",
            "Transform: ToTensor()\n",
            "Dataset CIFAR10\n",
            "    Number of datapoints: 10000\n",
            "    Root location: ./cifar-10-batches-py\n",
            "    Split: Test\n",
            "    StandardTransform\n",
            "Transform: ToTensor()\n",
            "train data size: torch.Size([32, 3, 32, 32])\n",
            "Adjusting learning rate of group 0 to 1.0000e-03.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/1563 [00:00<?, ?batch/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "85156346d92b4be3a44649916cd0a1d1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model saved!\n",
            "Adjusting learning rate of group 0 to 4.7830e-04.\n",
            "model saved!\n",
            "Adjusting learning rate of group 0 to 4.3047e-04.\n",
            "model saved!\n",
            "Adjusting learning rate of group 0 to 3.8742e-04.\n",
            "model saved!\n",
            "Adjusting learning rate of group 0 to 3.4868e-04.\n",
            "model saved!\n",
            "Adjusting learning rate of group 0 to 3.1381e-04.\n",
            "model saved!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Check the results"
      ],
      "metadata": {
        "id": "CMxI7UsrTkVe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib.pyplot import plot\n",
        "\n",
        "plot(losses)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "E2aDBAF-ATeR",
        "outputId": "c50b1d29-8edc-42fa-d23b-16703b77ec06"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f8121c902d0>]"
            ]
          },
          "metadata": {},
          "execution_count": 63
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib.pyplot import imshow\n",
        "%matplotlib inline\n",
        "def decode_img(img):\n",
        "    from PIL import Image\n",
        "    img = (img / 2 + 0.5).clamp(0, 1)\n",
        "    img = img.detach().cpu().permute(1, 2, 0).numpy()\n",
        "    img = (img * 255).round().astype('uint8')\n",
        "    pil_image = Image.fromarray(img)\n",
        "    return pil_image\n",
        "\n",
        "def adjust(sample):\n",
        "        # sample=np.expand_dims(np.asarray(sample), axis=0)\n",
        "        sample=sample.numpy()/ 1.0\n",
        "        # sample=sample.astype('float32')\n",
        "        sample = torch.from_numpy(sample).float() # Batch - RGB channel - WxH \n",
        "        sample = 2 * (sample - 0.5) # values between (-1, 1)\n",
        "        return sample\n"
      ],
      "metadata": {
        "id": "8XpNYhETpyvG"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_imgs = next(iter(test_data))[0]\n",
        "img=batch_imgs[0]"
      ],
      "metadata": {
        "id": "WNF-e-xiC-2P"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nu88VDINCZaJ",
        "outputId": "88bfbb07-4bfb-406d-e4b1-6d103b43e6d4"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cifar-10-batches-py  None_net.pth  sample_data\ttest1_net.pth  test2_net.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = torch.load(\"./None_net.pth\")\n",
        "vae.load_state_dict(checkpoint['model_state_dict'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zdp_u-z8mbzC",
        "outputId": "23d06d7a-ecf1-44a1-eb1f-78558c8165d7"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decode_img(img)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49
        },
        "id": "f_A8abVtqv0j",
        "outputId": "52641b0b-c3e3-44c1-9163-d9108bb3a343"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=32x32 at 0x7F812ACC6890>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAIAAAD8GO2jAAAHMUlEQVR4nDVW245ctxGsbjZ5OGdmR7sypDgBAtvI//9JPiAJggS5KLa0snbnds4h2Zc8zPqtAaKryEazqujP/+1E5G4eHuEIEAAiAoiIiADE/SACCAKIwAQiBgCAEAzoGO5ecgExADXziFyyIBwghFNYuL/1BIHorR1AgIMiIsLpXt3ZwgkBM1P98svP1/Plh59+ytMuIj59+qRuP/z4ozCCCKDwAN8xA0QgumMDuHN5wAEzHW4jwmws1hcOUMTp5fXrl+ftturyjUX6GMu6vn//Xm97AYCIOwrFGwPhXhEh7pQR4Ta0r61d+7YwuSSEttG39Xxezhce7fsPj7/7/kPb1t7lJBbjvL7+LAgE032SRAGPoDsRgd4uH3CEqba2nbVdMW5EHi7klFQPgvcfH7duTx8+PD69S/zdPB9Pp8vf/v7X5foiQaAgChCCwyI03H+bzv1lTETmRjFgA9rZNaxLQk1JfQ1odNuW9vz5s4jUXZ2qt94iwEJCYQxPrjQWW08So5QEhJmZ2bquOgZSoTynxJM2tOba5ypzprFd2uWsBtUAC0dQRJ12DL5er+56mGch1wzz5dTOz5Pf5omYqwdVkamWlsvlsm3ry7g9N1MiL8C8qyLRttvtetXhHlBzJmYKBKZpl/NUymRmpioTRpx/6a+fGZoKqyFjylNBxLJcTUdhkJAuN7JR55pLidC+deuDA8zcm6o6wnzZzudLna/HI/+2912e//WX0l6Sb9Nh33p/fHxkRlsurhpuiHDVtm4U8fBwkJID1FofQ909AkNt68ODelvQNkX69eXy+Hi8LSdHd4ecP//7qdhUhUJLzjq6bwvCEXBTN3V3EeRc81RZxDxqlbDlum5bs2UbQyMAdzBDhy/rcjqdOMX+UHonrrUww7SPthLFul5H21zdzdx9qDpCskR47z2lLHlKKTFjmN3Wtm5qdkdPJZeUQrJyGnXHSWh0FUpkpj56NwMnVwPASUAxVFtru92O2YeOgOLKKYmqtra5h2pYOBEQzMzmnpMej0KQgDMHFRZhsmYJGMP6t0sRSUKBDkBVW+8lFwNpb6bety2l0oetW1s3VY8AB4iZPaDDues0T2bm7kmQRQRUl8UobERct3VXp30tTE6Au7tF27qLjm1zdyMBxzDcFl227k5gRsAdAQTQGuE0glwSxNH7EDz98fXzc7+dHaYe174tzaaSJTE53KPpuqvFFF0dcMfoFn04WJjvIk6It08PTsQliTEbCRhJXlD94eN2a4wIDtPx66XV4rVkhsONCdsIixjqgILYzUVkqtl0DB3M6a65te6cuGs71CyFOYFA8vzPf+jlRdwLmzqMizO6U+E8tIeFMI+mpcjDw/62ttaciN1tbO7ud9l1AiX+8PGjFHl5/SoSwpIIBIhe/5eghwMdynRZ2uiDc/GIaxtwD/WcaBKeRKYsfeiyGQJgUjd4JBECu3lJZL1PRd4fnwLqNtyMiOQ4Z+GJdfMI5pwTHo5Pt9tyu14ZQHA4Cdgcy9qGGnPiVHb7QuzX2+KBu3OWWh4e5uPxXUoZ5OYjIsYwEQ5mZpku13PrHlKnLI1DGAB611LEXdbmQRrMdTeXnB/f79W1D93W7kAWUfjL7RWCXd0n4aB4Ob0+f/0mHA5vIsUCS1doa1++FOH9XK+3zd6s0haHmtWSC7Fq2xZHBMx7V/eoD7tDLYwYrcPfPDLUH3azcCgT29jU3J0QUaZ4dyiJ2V23tiFMpCx99G51EnOvE8+1BOJ661sfiWnelV2RCNO+hnYQicihTo+HWR6PO5bkxuZ8vn5zxG7K7w5SkgjjdD4xSynT0hwwZkoJOfN82IVjqv2uEofDfDzOFkZMcIBQpzJNU5IkT++eAKgDXH49LZfbFk61zFVSkt28ex3qTa2PcTdnjzAHghOLGcLBLCJTSoXJAQZDRIjQu4mztGUQQ90Y/Pju4bb103n7Ol2P+ymY5/389dvrdVvAXoV3BcxBiG3tgCII4CwTgdq6gQlgSdJHD4CJlE3GuIkwuxWkKYFZmumXl2+SHve1HIu8gnu3Wvj7x92x3gWJoUuASvaSfJ4iY4ne3eFOXDMBpqrhFBBsZ2UkYicJjZR4eBq92XrOMs+JqiTtcSj5UJL4AFG4+XpzCu+eoqdopAvzIE8MxrAAOBxwAmQnHHDAnUM4KFTY3h2mh0NJ4oVlnvMylpQyCxGYUwoQIgJMHQCbOZgjyN0BOJzeEqcTs5AI0z050n5Kj3Nx833J1xavV22qWwdCXm/mvlYhFiSRkhKDLpsp7zaT0+ITMyFJikLBzIQgMBEEFCB4uLnXxH/6w7F1//b6um0KlggpUo6Huozl0vptDRC7xz1yW9C8fyDm/3y5hQ8GC/nDLPt9rUVKAsMEYXc1ZyaBh14npt9/VwgliAMl14M5loXhBneiFMHdwiMQnnMBMHRmTu62dZjTy3nLYjUnEfwfNN6kf0puQWUAAAAASUVORK5CYII=\n"
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output=vae(adjust(torch.Tensor(img)[None, :]).to(device))[0]"
      ],
      "metadata": {
        "id": "gk7rLY7FqzbZ"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decode_img(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49
        },
        "id": "1Ykj1bp-oDTY",
        "outputId": "212943a7-9fc3-448f-f168-318cb097f264"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=32x32 at 0x7F8121CD8B10>"
            ],
            "image/png": "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\n"
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "FSsuvVVsmDHP"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jeb2mkyhm-1B",
        "outputId": "08b77fba-dbc9-43f0-a7e1-3afa1760577c"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cifar-10-batches-py  None_net.pth  sample_data\ttest1_net.pth  test2_net.pth\n"
          ]
        }
      ]
    }
  ]
}