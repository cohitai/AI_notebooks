{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "4dc131b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ToDo: add training graphs !\n",
    "#ToDo: check microGrad and TinyGrad\n",
    "#warmup_tokens and final_token \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "75301d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F # Cross-Entropy\n",
    "import pytorch_lightning as pl\n",
    "import os\n",
    "\n",
    "from tqdm import tqdm # bar\n",
    "import numpy as np\n",
    "import torch.optim as optim # Adam optimizer\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.utils.data.dataloader import DataLoader \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "433e066f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model parameters\n",
    "\n",
    "ndigit=2 #1,2,3,...\n",
    "n_embd=128\n",
    "vocab_size=10\n",
    "block_size=3*ndigit\n",
    "n_layer=2\n",
    "n_heads=4\n",
    "\n",
    "embd_pdrop = 0.1\n",
    "resid_pdrop = 0.1\n",
    "attn_pdrop = 0.1\n",
    "\n",
    "DEVICE=\"cpu\"\n",
    "\n",
    "#training parameters\n",
    "\n",
    "bs = 512\n",
    "lr=6e-4\n",
    "Max_Epochs=10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "f129204a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "f5fb9727",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset():\n",
    "    ret = []\n",
    "    for i in range(10**ndigit):\n",
    "        for j in range(10**ndigit):\n",
    "            s = i+j\n",
    "            ret.append([i//10, i%10, j//10, j%10, s//100, (s//10)%10, s%10])\n",
    "    return ret\n",
    "ds = make_dataset()\n",
    "random.shuffle(ds)\n",
    "ds = np.array(ds)\n",
    "ds_X = ds[:, 0:6]\n",
    "ds_Y = np.copy(ds[:, 1:])\n",
    "ds_X_train, ds_X_test = ds_X[0:8000], ds_X[8000:]\n",
    "ds_Y_train, ds_Y_test = ds_Y[0:8000], ds_Y[8000:]\n",
    "\n",
    "#wrapper (ToDo: simplify & get rid of it.)\n",
    "from torch.utils.data import Dataset\n",
    "class AdditionDataset(Dataset):\n",
    "    def __init__(self,X,Y):\n",
    "        self.X=X\n",
    "        self.Y=Y\n",
    "    \n",
    "    \n",
    "    def __len__(self,):\n",
    "        return self.X.shape[0]\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        self.x = torch.tensor(self.X[idx], dtype=torch.long)\n",
    "        self.y = torch.tensor(self.Y[idx], dtype=torch.long) # predict the next token in the sequence\n",
    "        self.y[:ndigit*2-1] = -100 # we will only train in the output locations. -100 will mask loss to zero\n",
    "        return self.x, self.y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "300ae173",
   "metadata": {},
   "outputs": [],
   "source": [
    "AdditionDataset(ds_X_train,ds_Y_train)\n",
    "\n",
    "train_dataset = AdditionDataset(ds_X_train,ds_Y_train)\n",
    "test_dataset = AdditionDataset(ds_X_test,ds_Y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "c32bed6f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9 2 4 5 1 3]\n"
     ]
    }
   ],
   "source": [
    "print(ds_X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "51f4b47d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 4 5 1 3 7]\n"
     ]
    }
   ],
   "source": [
    "print(ds_Y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5db82de",
   "metadata": {},
   "source": [
    "# torch.nn Module for GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "68ca1c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedder(nn.Module):\n",
    "    \"\"\" simple aux class to embed data entry \"\"\"  \n",
    "    def __init__(self, block_size, vocab_size, n_embd):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(vocab_size, n_embd) # -> (BS,T,C)\n",
    "        self.pos_emb = nn.Parameter(torch.zeros(1, block_size, n_embd)) # ->(1,T,C)\n",
    "   \n",
    "    def __call__(self,x):\n",
    "        b, t = torch.Tensor(x).shape\n",
    "        token_embeddings = self.tok_emb( torch.Tensor(x)) # each index maps to a (learnable) vector\n",
    "        position_embeddings = self.pos_emb # each index maps to a (learnable) vector\n",
    "        return token_embeddings+position_embeddings\n",
    "\n",
    "\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        assert n_embd % n_heads == 0\n",
    "        self.proj_dim = n_embd // n_heads\n",
    "        self.attn_drop = nn.Dropout(attn_pdrop)\n",
    "        self.resid_drop = nn.Dropout(resid_pdrop)\n",
    "        # output projection\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        # causal mask to ensure that attention is only applied to the left in the input sequence\n",
    "        self.register_buffer(\"mask\", torch.tril(torch.ones(block_size, block_size))\n",
    "                                     .view(1, 1, block_size, block_size))      \n",
    "\n",
    "        # 4 * embed_dim**2 weights\n",
    "        \n",
    "        self.query = nn.Linear(n_embd,n_embd)\n",
    "        self.key = nn.Linear(n_embd,n_embd)\n",
    "        self.value = nn.Linear(n_embd,n_embd)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        k = self.key(x).view(B, T, n_heads, self.proj_dim).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = self.query(x).view(B, T, n_heads, self.proj_dim).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = self.value(x).view(B, T, n_heads, self.proj_dim).transpose(1, 2) # (B, nh, T, hs)\n",
    "\n",
    "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "        att = att.masked_fill(self.mask[:,:,:T,:T] == 0, float('-inf'))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        att = self.attn_drop(att)\n",
    "        y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "\n",
    "        # output projection\n",
    "        y = self.resid_drop(self.proj(y))\n",
    "        return y\n",
    "\n",
    "    \n",
    "class Block(nn.Module):\n",
    "    \"\"\" an unassuming Transformer block \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "        self.attn = MultiHeadSelfAttention()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(resid_pdrop),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "    \n",
    "class GPT(nn.Module):\n",
    "    \"\"\"  the full GPT language model, with a context size of block_size \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # embedder\n",
    "        self.Embedder = Embedder(block_size,vocab_size,n_embd)\n",
    "        # transformer\n",
    "        self.blocks = nn.Sequential(*[Block() for _ in range(n_layer)])\n",
    "        # dropout\n",
    "        self.drop = nn.Dropout(embd_pdrop)\n",
    "        # layer normalizations\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "        # head\n",
    "        self.head = nn.Linear(n_embd, vocab_size, bias=False)\n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "        print(\"number of parameters: %e\", sum(p.numel() for p in self.parameters())) \n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, (nn.Linear, nn.Embedding)):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if isinstance(module, nn.Linear) and module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            torch.nn.init.zeros_(module.bias)\n",
    "            torch.nn.init.ones_(module.weight)\n",
    "        elif isinstance(module, GPT):\n",
    "            torch.nn.init.normal_(module.Embedder.pos_emb, mean=0.0, std=0.02)\n",
    "            \n",
    "    def configure_optimizers(self, train_config):\n",
    "        \"\"\"\n",
    "        This long function is unfortunately doing something very simple and is being very defensive:\n",
    "        We are separating out all parameters of the model into two buckets: those that will experience\n",
    "        weight decay for regularization and those that won't (biases, and layernorm/embedding weights).\n",
    "        We are then returning the PyTorch optimizer object.\n",
    "        \"\"\"\n",
    "\n",
    "        # separate out all parameters to those that will and won't experience regularizing weight decay\n",
    "        decay = set()\n",
    "        no_decay = set()\n",
    "        whitelist_weight_modules = (torch.nn.Linear, )\n",
    "        blacklist_weight_modules = (torch.nn.LayerNorm, torch.nn.Embedding)\n",
    "        for mn, m in self.named_modules():\n",
    "            for pn, p in m.named_parameters():\n",
    "                fpn = '%s.%s' % (mn, pn) if mn else pn # full param name\n",
    "\n",
    "                if pn.endswith('bias'):\n",
    "                    # all biases will not be decayed\n",
    "                    no_decay.add(fpn)\n",
    "                elif pn.endswith('weight') and isinstance(m, whitelist_weight_modules):\n",
    "                    # weights of whitelist modules will be weight decayed\n",
    "                    decay.add(fpn)\n",
    "                elif pn.endswith('weight') and isinstance(m, blacklist_weight_modules):\n",
    "                    # weights of blacklist modules will NOT be weight decayed\n",
    "                    no_decay.add(fpn)\n",
    "\n",
    "        # special case the position embedding parameter in the root GPT module as not decayed\n",
    "        no_decay.add('Embedder.pos_emb')\n",
    "\n",
    "        # validate that we considered every parameter\n",
    "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
    "        inter_params = decay & no_decay\n",
    "        union_params = decay | no_decay\n",
    "        assert len(inter_params) == 0, \"parameters %s made it into both decay/no_decay sets!\" % (str(inter_params), )\n",
    "        assert len(param_dict.keys() - union_params) == 0, \"parameters %s were not separated into either decay/no_decay set!\" \\\n",
    "                                                    % (str(param_dict.keys() - union_params), )\n",
    "\n",
    "        # create the pytorch optimizer object\n",
    "        optim_groups = [\n",
    "            {\"params\": [param_dict[pn] for pn in sorted(list(decay))], \"weight_decay\": train_config.weight_decay},\n",
    "            {\"params\": [param_dict[pn] for pn in sorted(list(no_decay))], \"weight_decay\": 0.0},\n",
    "        ]\n",
    "        optimizer = torch.optim.AdamW(optim_groups, lr=train_config.learning_rate, betas=train_config.betas)\n",
    "        return optimizer\n",
    "        \n",
    "    def forward(self,x,y=None):\n",
    "        \n",
    "        #cast numpy-> torch.Tensor\n",
    "        x = torch.Tensor(x).to(DEVICE).long()\n",
    "        if y is not None:\n",
    "            y = torch.Tensor(y).to(DEVICE).long()\n",
    "        \n",
    "        x=self.Embedder(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.head(x)\n",
    "        \n",
    "         # if we are given some desired targets <y>, also calculate the loss\n",
    "        loss = None\n",
    "        if y is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), y.view(-1))\n",
    "\n",
    "        return logits, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7afdc5c",
   "metadata": {},
   "source": [
    "# Trainer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "a532d428",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TrainerConfig: #configuration class\n",
    "    # optimization parameters\n",
    "    max_epochs = 10\n",
    "    batch_size = 64\n",
    "    learning_rate = 3e-4\n",
    "    betas = (0.9, 0.95)\n",
    "    grad_norm_clip = 1.0\n",
    "    weight_decay = 0.1 # only applied on matmul weights\n",
    "    # learning rate decay params: linear warmup followed by cosine decay to 10% of original\n",
    "    lr_decay = False\n",
    "    warmup_tokens = 375e6 # these two numbers come from the GPT-3 paper, but may not be good defaults elsewhere\n",
    "    final_tokens = 260e9 # (at what point we reach 10% of original LR)\n",
    "    # checkpoint settings\n",
    "    ckpt_path = None\n",
    "    num_workers = 0 # for DataLoader\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        for k,v in kwargs.items():\n",
    "            setattr(self, k, v)    \n",
    "    \n",
    "class Trainer:\n",
    "\n",
    "    def __init__(self, model, train_dataset, test_dataset, config):\n",
    "        self.model = model\n",
    "        self.train_dataset = train_dataset\n",
    "        self.test_dataset = test_dataset\n",
    "        self.config = config\n",
    "\n",
    "        # take over whatever gpus are on the system\n",
    "        self.device = 'cpu'\n",
    "\n",
    "    def save_checkpoint(self):\n",
    "        # DataParallel wrappers keep raw model object in .module attribute\n",
    "        raw_model = self.model.module if hasattr(self.model, \"module\") else self.model\n",
    "        torch.save(raw_model.state_dict(), self.config.ckpt_path)\n",
    "\n",
    "    def train(self):\n",
    "        model, config = self.model, self.config\n",
    "        raw_model = model.module if hasattr(self.model, \"module\") else model\n",
    "        optimizer = raw_model.configure_optimizers(config)\n",
    "\n",
    "        def run_epoch(loader, is_train):\n",
    "            model.train(is_train)\n",
    "\n",
    "            losses = []\n",
    "            pbar = tqdm(enumerate(loader), total=len(loader)) if is_train else enumerate(loader)\n",
    "            for it, (x, y) in pbar:\n",
    "\n",
    "                # place data on the correct device\n",
    "                x = x.to(self.device)\n",
    "                y = y.to(self.device)\n",
    "\n",
    "                # forward the model\n",
    "                with torch.set_grad_enabled(is_train):\n",
    "                    logits, loss = model(x, y)\n",
    "                    loss = loss.mean() # collapse all losses if they are scattered on multiple gpus\n",
    "                    losses.append(loss.item())\n",
    "                \n",
    "                if is_train:\n",
    "                    # backprop and update the parameters\n",
    "                    model.zero_grad()\n",
    "                    loss.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), config.grad_norm_clip)\n",
    "                    optimizer.step()\n",
    "\n",
    "                    # decay the learning rate based on our progress\n",
    "                    if config.lr_decay:\n",
    "                        self.tokens += (y >= 0).sum() # number of tokens processed this step (i.e. label is not -100)\n",
    "                        if self.tokens < config.warmup_tokens:\n",
    "                            # linear warmup\n",
    "                            lr_mult = float(self.tokens) / float(max(1, config.warmup_tokens))\n",
    "                        else:\n",
    "                            # cosine learning rate decay\n",
    "                            progress = float(self.tokens - config.warmup_tokens) / float(max(1, config.final_tokens - config.warmup_tokens))\n",
    "                            lr_mult = max(0.1, 0.5 * (1.0 + math.cos(math.pi * progress)))\n",
    "                        lr = config.learning_rate * lr_mult\n",
    "                        for param_group in optimizer.param_groups:\n",
    "                            param_group['lr'] = lr\n",
    "                    else:\n",
    "                        lr = config.learning_rate\n",
    "\n",
    "                    # report progress\n",
    "                    pbar.set_description(f\"epoch {epoch+1} iter {it}: train loss {loss.item():.5f}. lr {lr:e}\")\n",
    "\n",
    "            if not is_train:\n",
    "                test_loss = float(np.mean(losses))\n",
    "                return test_loss\n",
    "\n",
    "        best_loss = float('inf')\n",
    "        self.tokens = 0 # counter used for learning rate decay\n",
    "\n",
    "        train_loader = DataLoader(\n",
    "            self.train_dataset,\n",
    "            shuffle=True,\n",
    "            pin_memory=True,\n",
    "            batch_size=config.batch_size,\n",
    "            num_workers=config.num_workers\n",
    "        )\n",
    "        if self.test_dataset is not None:\n",
    "            test_loader = DataLoader(\n",
    "                self.test_dataset,\n",
    "                shuffle=True,\n",
    "                pin_memory=True,\n",
    "                batch_size=config.batch_size,\n",
    "                num_workers=config.num_workers\n",
    "            )\n",
    "\n",
    "        for epoch in range(config.max_epochs):\n",
    "            run_epoch(train_loader, is_train=True)\n",
    "            if self.test_dataset is not None:\n",
    "                test_loss = run_epoch(test_loader, is_train=False)\n",
    "\n",
    "            # supports early stopping based on the test loss, or just save always if no test set is provided\n",
    "            good_model = self.test_dataset is None or test_loss < best_loss\n",
    "            if self.config.ckpt_path is not None and good_model:\n",
    "                best_loss = test_loss\n",
    "                self.save_checkpoint()\n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d77eb1",
   "metadata": {},
   "source": [
    "# run "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "0d06079b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tconf = TrainerConfig(max_epochs=10, batch_size=512, learning_rate=6e-4,\n",
    "                      lr_decay=True, warmup_tokens=1024, final_tokens=50*len(ds_X_train)*(ndigit+1),\n",
    "                      num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "fa4771ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: %e 400128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1 iter 15: train loss 1.63915. lr 5.994565e-04: 100%|██████████████████████████████████| 16/16 [00:01<00:00, 12.06it/s]\n",
      "epoch 2 iter 15: train loss 1.42369. lr 5.977303e-04: 100%|██████████████████████████████████| 16/16 [00:01<00:00, 12.08it/s]\n",
      "epoch 3 iter 15: train loss 1.27394. lr 5.948270e-04: 100%|██████████████████████████████████| 16/16 [00:01<00:00, 11.95it/s]\n",
      "epoch 4 iter 15: train loss 1.22703. lr 5.907582e-04: 100%|██████████████████████████████████| 16/16 [00:01<00:00, 11.85it/s]\n",
      "epoch 5 iter 15: train loss 1.17454. lr 5.855400e-04: 100%|██████████████████████████████████| 16/16 [00:01<00:00, 11.67it/s]\n",
      "epoch 6 iter 15: train loss 0.99136. lr 5.791929e-04: 100%|██████████████████████████████████| 16/16 [00:01<00:00, 11.83it/s]\n",
      "epoch 7 iter 15: train loss 0.71794. lr 5.717421e-04: 100%|██████████████████████████████████| 16/16 [00:01<00:00, 11.67it/s]\n",
      "epoch 8 iter 15: train loss 0.63615. lr 5.632171e-04: 100%|██████████████████████████████████| 16/16 [00:01<00:00, 11.66it/s]\n",
      "epoch 9 iter 1: train loss 0.61933. lr 5.620500e-04:  12%|████▌                               | 2/16 [00:00<00:01,  7.62it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [234]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m GPT()\n\u001b[1;32m      3\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(model, train_dataset, test_dataset, tconf)\n\u001b[0;32m----> 4\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [230]\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    100\u001b[0m     test_loader \u001b[38;5;241m=\u001b[39m DataLoader(\n\u001b[1;32m    101\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest_dataset,\n\u001b[1;32m    102\u001b[0m         shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    105\u001b[0m         num_workers\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_workers\n\u001b[1;32m    106\u001b[0m     )\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(config\u001b[38;5;241m.\u001b[39mmax_epochs):\n\u001b[0;32m--> 109\u001b[0m     \u001b[43mrun_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    111\u001b[0m         test_loss \u001b[38;5;241m=\u001b[39m run_epoch(test_loader, is_train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Input \u001b[0;32mIn [230]\u001b[0m, in \u001b[0;36mTrainer.train.<locals>.run_epoch\u001b[0;34m(loader, is_train)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_train:\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;66;03m# backprop and update the parameters\u001b[39;00m\n\u001b[1;32m     61\u001b[0m     model\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 62\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m     torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), config\u001b[38;5;241m.\u001b[39mgrad_norm_clip)\n\u001b[1;32m     64\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/projects/pytorch_nightly/lib/python3.8/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/pytorch_nightly/lib/python3.8/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = GPT()\n",
    "\n",
    "trainer = Trainer(model, train_dataset, test_dataset, tconf)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "c9b68d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "ret=np.argmax(model(train_dataset[:][0])[0].detach().numpy(),axis=2)[:,-3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "2948af0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "l=[]\n",
    "for i in range(len(train_dataset)):\n",
    "    l.append(train_dataset[i][1][-3:].detach().numpy())\n",
    "l=np.asarray(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "87c12de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct=sum(ret[:, -1]==l[:, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "fab3fb35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct/len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "9be03412",
   "metadata": {},
   "outputs": [],
   "source": [
    "ret=np.argmax(model(test_dataset[:][0])[0].detach().numpy(),axis=2)[:,-3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "4dc60324",
   "metadata": {},
   "outputs": [],
   "source": [
    "l=[]\n",
    "for i in range(len(test_dataset)):\n",
    "    l.append(test_dataset[i][1][-3:].detach().numpy())\n",
    "l=np.asarray(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "d6df8df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct=sum(ret[:, -1]==l[:, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "3156ec15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct/len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdab51b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4242fd28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b136cc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT().configure_optimizers(tconf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "311960e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B,T inpute size, C embedded dimension: 8000 6 128\n",
      "dtype: torch.float32\n"
     ]
    }
   ],
   "source": [
    "x = ds_X_train \n",
    "x = Embedder(block_size,vocab_size,n_embd)(x)\n",
    "B,T,C = x.size() \n",
    "\n",
    "print(\"B,T inpute size, C embedded dimension:\",B,T,C)\n",
    "print(\"dtype:\",x.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "ddbfff4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8000, 6, 128])"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Block()(x).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "4b6e309f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = ds_X_train[1].reshape(1,-1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "bf5d4392",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 6)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "dd1d03b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ToDo Dataloader not compatible\n",
    "# K,V,Q Layer better understanding ! multihead process.. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0eb2f5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
